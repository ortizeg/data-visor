---
phase: 06-error-analysis-similarity
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/error_analysis.py
  - app/models/error_analysis.py
  - app/routers/statistics.py
  - frontend/src/types/error-analysis.ts
  - frontend/src/hooks/use-error-analysis.ts
  - frontend/src/components/stats/error-analysis-panel.tsx
  - frontend/src/components/stats/error-samples-grid.tsx
  - frontend/src/components/stats/stats-dashboard.tsx
autonomous: true

must_haves:
  truths:
    - "User can view error summary cards showing TP, Hard FP, Label Error, FN counts"
    - "User can see per-class error distribution as a stacked bar chart"
    - "User can click an error type to see sample thumbnails with that error"
    - "IoU and confidence sliders control error categorization thresholds"
  artifacts:
    - path: "app/services/error_analysis.py"
      provides: "Per-detection error categorization using IoU matching"
      exports: ["categorize_errors"]
    - path: "app/models/error_analysis.py"
      provides: "Pydantic models for error summary and per-class errors"
      exports: ["ErrorSummary", "ErrorAnalysisResponse", "PerClassErrors", "ErrorSample"]
    - path: "app/routers/statistics.py"
      provides: "GET /datasets/{id}/error-analysis endpoint"
      contains: "error_analysis"
    - path: "frontend/src/components/stats/error-analysis-panel.tsx"
      provides: "Error Analysis sub-tab with summary cards, bar chart, and sample grid"
    - path: "frontend/src/components/stats/stats-dashboard.tsx"
      provides: "Third sub-tab 'Error Analysis' alongside Overview and Evaluation"
  key_links:
    - from: "app/routers/statistics.py"
      to: "app/services/error_analysis.py"
      via: "categorize_errors function call"
      pattern: "categorize_errors"
    - from: "app/services/error_analysis.py"
      to: "app/services/evaluation.py"
      via: "reuses _load_detections and _compute_iou_matrix patterns"
      pattern: "_compute_iou_matrix"
    - from: "frontend/src/hooks/use-error-analysis.ts"
      to: "/datasets/{id}/error-analysis"
      via: "TanStack useQuery fetch"
      pattern: "error-analysis"
    - from: "frontend/src/components/stats/stats-dashboard.tsx"
      to: "frontend/src/components/stats/error-analysis-panel.tsx"
      via: "sub-tab rendering"
      pattern: "ErrorAnalysisPanel"
---

<objective>
Build per-detection error categorization that classifies predictions into True Positive, Hard False Positive, Label Error, and False Negative categories using IoU matching, then render an Error Analysis sub-tab in the statistics dashboard.

Purpose: Users need to understand WHY their model fails -- not just mAP numbers. Error categorization surfaces systematic failure patterns (background hallucinations, class confusion, missed detections) so users can prioritize data collection and model improvement.

Output: Backend error analysis service + endpoint, frontend Error Analysis tab with summary cards, per-class stacked bar chart, and clickable error sample grid.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Error categorization algorithm and architecture patterns
@.planning/phases/06-error-analysis-similarity/06-RESEARCH.md

# Existing evaluation service with IoU matching logic to extend
@app/services/evaluation.py
@app/models/evaluation.py

# Statistics router to add error-analysis endpoint to
@app/routers/statistics.py

# Stats dashboard to add Error Analysis sub-tab to
@frontend/src/components/stats/stats-dashboard.tsx
@frontend/src/components/stats/evaluation-panel.tsx

# Existing hook pattern to follow
@frontend/src/hooks/use-evaluation.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Error categorization service, models, and API endpoint</name>
  <files>
    app/services/error_analysis.py
    app/models/error_analysis.py
    app/routers/statistics.py
  </files>
  <action>
    **1. Create `app/models/error_analysis.py`** with Pydantic response models:

    - `ErrorSample`: sample_id (str), error_type (str), category_name (str), confidence (float | None)
    - `PerClassErrors`: class_name (str), tp (int), hard_fp (int), label_error (int), fn (int)
    - `ErrorSummary`: true_positives (int), hard_false_positives (int), label_errors (int), false_negatives (int)
    - `ErrorAnalysisResponse`: summary (ErrorSummary), per_class (list[PerClassErrors]), samples_by_type (dict[str, list[ErrorSample]]) -- keys are "tp", "hard_fp", "label_error", "false_negative", values are lists of ErrorSample capped at 50 per type

    **2. Create `app/services/error_analysis.py`** with the error categorization algorithm:

    - Reuse `_load_detections` from evaluation.py (import it or extract to shared helper). The function loads GT and predictions grouped by sample_id.
    - Reuse the `_compute_iou_matrix` function from evaluation.py (import it directly).
    - Implement `categorize_errors(cursor, dataset_id, source, iou_threshold, conf_threshold) -> ErrorAnalysisResponse`:
      1. Call `_load_detections(cursor, dataset_id, source)` to get gt_by_sample, pred_by_sample, class_names
      2. For each sample_id in the union of GT and prediction keys:
         - Filter predictions by confidence >= conf_threshold
         - Sort predictions by confidence descending (greedy matching order matters)
         - For each prediction, compute IoU against all GT boxes using `_compute_iou_matrix`
         - Classify: TP (IoU >= threshold AND class matches AND GT not already matched), Label Error (IoU >= threshold AND class mismatch), Hard FP (IoU < threshold for all GT)
         - Track matched GT indices per sample
         - After all predictions processed: unmatched GT boxes are False Negatives
      3. Aggregate into ErrorSummary (total counts) and PerClassErrors (per-class breakdown)
      4. Collect ErrorSample lists grouped by error type, capped at 50 per type (avoid huge payloads)
      5. Return ErrorAnalysisResponse

    Key details:
    - Import `_load_detections` and `_compute_iou_matrix` from `app.services.evaluation` (make them importable by keeping the underscore prefix -- they are internal but shared within the services package). If this causes import issues, copy the `_compute_iou_matrix` function instead.
    - Use numpy arrays for bbox operations (same pattern as evaluation.py)
    - Handle edge cases: no GT (all preds are Hard FP), no preds (all GT are FN), empty dataset

    **3. Add endpoint to `app/routers/statistics.py`:**

    ```python
    @router.get("/{dataset_id}/error-analysis")
    def get_error_analysis(
        dataset_id: str,
        source: str = Query("prediction"),
        iou_threshold: float = Query(0.5, ge=0.1, le=1.0),
        conf_threshold: float = Query(0.25, ge=0.0, le=1.0),
        db: DuckDBRepo = Depends(get_db),
    ) -> ErrorAnalysisResponse:
    ```

    Follow the same pattern as `get_evaluation`: verify dataset exists, verify source has annotations, call `categorize_errors`, return response. Import ErrorAnalysisResponse model and categorize_errors function.
  </action>
  <verify>
    Start the backend (`cd /path/to/project && uv run uvicorn app.main:app --reload`) and test:
    ```bash
    curl "http://localhost:8000/datasets/{DATASET_ID}/error-analysis?source=prediction&iou_threshold=0.5&conf_threshold=0.25" | python -m json.tool
    ```
    Response must include: summary (with tp/hard_fp/label_error/fn counts), per_class (list of class breakdowns), samples_by_type (dict with error type keys). Verify that summary counts add up correctly (tp + hard_fp + label_error should equal total predictions above conf_threshold; fn should equal unmatched GT count).
  </verify>
  <done>
    GET /datasets/{id}/error-analysis returns ErrorAnalysisResponse with correct error category counts. IoU and confidence thresholds affect categorization. Edge cases (no predictions, no GT) handled gracefully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Error Analysis sub-tab in statistics dashboard</name>
  <files>
    frontend/src/types/error-analysis.ts
    frontend/src/hooks/use-error-analysis.ts
    frontend/src/components/stats/error-analysis-panel.tsx
    frontend/src/components/stats/error-samples-grid.tsx
    frontend/src/components/stats/stats-dashboard.tsx
  </files>
  <action>
    **1. Create `frontend/src/types/error-analysis.ts`:**

    TypeScript types mirroring the backend Pydantic models:
    - `ErrorSample`: { sample_id: string; error_type: string; category_name: string; confidence: number | null }
    - `PerClassErrors`: { class_name: string; tp: number; hard_fp: number; label_error: number; fn: number }
    - `ErrorSummary`: { true_positives: number; hard_false_positives: number; label_errors: number; false_negatives: number }
    - `ErrorAnalysisResponse`: { summary: ErrorSummary; per_class: PerClassErrors[]; samples_by_type: Record<string, ErrorSample[]> }

    **2. Create `frontend/src/hooks/use-error-analysis.ts`:**

    Follow the pattern from `use-evaluation.ts`:
    - `useErrorAnalysis(datasetId, source, iouThreshold, confThreshold)` -> useQuery hook
    - Query key: `["error-analysis", datasetId, source, iouThreshold, confThreshold]`
    - Fetch URL: `${API_BASE}/datasets/${datasetId}/error-analysis?source=${source}&iou_threshold=${iouThreshold}&conf_threshold=${confThreshold}`
    - staleTime: 30_000 (30s, since results depend on thresholds)
    - enabled: !!datasetId

    **3. Create `frontend/src/components/stats/error-analysis-panel.tsx`:**

    The Error Analysis panel with:
    - **IoU and confidence threshold sliders** (same as EvaluationPanel -- range inputs with debounced state, IoU 0.1-1.0 default 0.5, confidence 0.0-1.0 default 0.25). Use local state + debounce pattern matching EvaluationPanel.
    - **Summary cards row** (4 cards in a grid): True Positives (green), Hard False Positives (red), Label Errors (amber/yellow), False Negatives (orange). Each card shows count and percentage of total detections.
    - **Per-class error distribution stacked bar chart** using Recharts BarChart:
      - X axis: class names, Y axis: count
      - 4 stacked bars per class: tp (green-500), hard_fp (red-500), label_error (amber-500), fn (orange-500)
      - Use Recharts `<BarChart>`, `<Bar stackId="errors">`, `<XAxis>`, `<YAxis>`, `<Tooltip>`, `<Legend>`
    - **Error samples section**: show a row of clickable thumbnails for each error type (hard_fp, label_error, false_negative -- skip TP since those are correct). Use the ErrorSamplesGrid sub-component. Each thumbnail is clickable to open the SampleModal (via `useUIStore.openDetailModal(sampleId)`).

    **4. Create `frontend/src/components/stats/error-samples-grid.tsx`:**

    A compact thumbnail grid for a specific error type:
    - Props: `{ title: string; errorType: string; samples: ErrorSample[]; datasetId: string; color: string }`
    - Renders a section heading with the error type name and count badge
    - Grid of thumbnail images (4-6 columns) using existing `thumbnailUrl()` helper from `@/lib/api`
    - Each thumbnail shows the category_name and confidence (if available) as a small overlay badge
    - Clicking a thumbnail calls `useUIStore.openDetailModal(sample.sample_id)`
    - If no samples, show "No {type} detections" message

    **5. Update `frontend/src/components/stats/stats-dashboard.tsx`:**

    - Add "Error Analysis" as a third sub-tab alongside "Overview" and "Evaluation"
    - Update the `SubTab` type: `type SubTab = "overview" | "evaluation" | "error_analysis"`
    - Add the tab button (disabled when no predictions, same as Evaluation tab)
    - When `activeTab === "error_analysis"`, render `<ErrorAnalysisPanel datasetId={datasetId} />`
    - Import ErrorAnalysisPanel

    Visual design:
    - Follow the existing dark/light theme patterns (zinc-based colors, border-zinc-200/700)
    - Summary cards match the AnnotationSummary card style
    - Chart uses the same rounded-lg border pattern as ClassDistribution
  </action>
  <verify>
    1. Navigate to a dataset with predictions loaded
    2. Click the "Statistics" tab, then the "Error Analysis" sub-tab
    3. Verify summary cards show TP, Hard FP, Label Error, FN counts
    4. Verify the stacked bar chart shows per-class error distribution
    5. Verify clicking a thumbnail opens the SampleModal
    6. Adjust IoU and confidence sliders -- verify counts update
    7. Check TypeScript compiles: `cd frontend && npx tsc --noEmit`
  </verify>
  <done>
    Error Analysis sub-tab renders in the statistics dashboard with summary cards (color-coded counts), per-class stacked bar chart, and clickable error sample thumbnails. IoU/confidence sliders control the categorization thresholds. No TypeScript errors.
  </done>
</task>

</tasks>

<verification>
1. Backend: `curl` the error-analysis endpoint with various iou_threshold and conf_threshold values. Verify counts change appropriately (lower IoU -> more TPs, lower conf -> more detections categorized).
2. Frontend: Navigate to Statistics > Error Analysis tab. Cards show counts. Bar chart renders per-class breakdown. Thumbnails are clickable. Sliders adjust results.
3. Edge cases: Dataset with no predictions shows disabled Error Analysis tab. Dataset with predictions but no GT shows all predictions as Hard FP.
4. TypeScript: `cd frontend && npx tsc --noEmit` passes without errors.
</verification>

<success_criteria>
- Error Analysis sub-tab visible in statistics dashboard when predictions exist
- Summary cards display TP, Hard FP, Label Error, FN counts with correct color coding
- Stacked bar chart shows per-class error distribution
- Error sample thumbnails are visible and clickable (opens detail modal)
- IoU and confidence threshold sliders affect error categorization
- API endpoint returns correct ErrorAnalysisResponse structure
</success_criteria>

<output>
After completion, create `.planning/phases/06-error-analysis-similarity/06-01-SUMMARY.md`
</output>
