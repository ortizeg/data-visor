---
phase: 16-classification-evaluation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/ingestion/classification_prediction_parser.py
  - app/services/classification_evaluation.py
  - app/services/classification_error_analysis.py
  - app/models/classification_evaluation.py
  - app/models/prediction.py
  - app/routers/datasets.py
  - app/routers/statistics.py
  - app/routers/_run_name.py
autonomous: true

must_haves:
  truths:
    - "Classification predictions can be imported via POST /datasets/{id}/predictions with format=classification_jsonl"
    - "GET /datasets/{id}/evaluation returns accuracy, F1, confusion matrix, per-class P/R/F1 for classification datasets"
    - "GET /datasets/{id}/confusion-cell-samples returns sample IDs for a (gt_class, pred_class) pair without IoU matching"
    - "GET /datasets/{id}/error-analysis returns correct/misclassified/missing counts for classification datasets"
  artifacts:
    - path: "app/ingestion/classification_prediction_parser.py"
      provides: "JSONL prediction parser with sentinel bbox, filename-to-sample_id lookup"
    - path: "app/services/classification_evaluation.py"
      provides: "compute_classification_evaluation function returning accuracy, F1, confusion matrix, per-class metrics"
    - path: "app/services/classification_error_analysis.py"
      provides: "classify_errors function returning correct/misclassified/missing per sample"
    - path: "app/models/classification_evaluation.py"
      provides: "ClassificationEvaluationResponse, ClassificationPerClassMetrics Pydantic models"
  key_links:
    - from: "app/routers/datasets.py"
      to: "app/ingestion/classification_prediction_parser.py"
      via: "format == 'classification_jsonl' branch in import_predictions"
      pattern: "classification_jsonl"
    - from: "app/routers/statistics.py"
      to: "app/services/classification_evaluation.py"
      via: "dataset_type == 'classification' check in get_evaluation"
      pattern: "compute_classification_evaluation"
    - from: "app/routers/statistics.py"
      to: "app/services/classification_error_analysis.py"
      via: "dataset_type == 'classification' check in get_error_analysis"
      pattern: "classify_errors"
---

<objective>
Build classification prediction import, evaluation metrics, confusion matrix, and error analysis backend services.

Purpose: Enable classification model evaluation by providing prediction import, metric computation (accuracy, F1, confusion matrix, per-class P/R/F1), and error categorization (correct/misclassified/missing) -- all routed from existing endpoints based on dataset_type.

Output: Classification prediction parser, evaluation service, error analysis service, Pydantic response models, and updated API routing.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-classification-evaluation/16-RESEARCH.md
@.planning/phases/15-classification-ingestion-display/15-01-SUMMARY.md

Key existing files to reference:
@app/ingestion/classification_jsonl_parser.py  -- GT parser pattern to follow
@app/ingestion/prediction_parser.py            -- COCO prediction parser for pattern reference
@app/services/evaluation.py                    -- Detection evaluation (do NOT modify)
@app/services/error_analysis.py                -- Detection error analysis (do NOT modify)
@app/models/evaluation.py                      -- Detection evaluation response models
@app/models/error_analysis.py                  -- Detection error analysis response models
@app/models/prediction.py                      -- PredictionImportRequest (add format option)
@app/routers/datasets.py                       -- import_predictions endpoint (add classification branch)
@app/routers/statistics.py                     -- evaluation + error-analysis endpoints (add routing)
@app/routers/_run_name.py                      -- run_name derivation (add classification_jsonl)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Classification prediction parser and import endpoint</name>
  <files>
    app/ingestion/classification_prediction_parser.py
    app/models/prediction.py
    app/routers/datasets.py
    app/routers/_run_name.py
  </files>
  <action>
    1. Create `app/ingestion/classification_prediction_parser.py`:
       - Class `ClassificationPredictionParser` following the same pattern as `ClassificationJSONLParser` (from Phase 15).
       - Flexible key lookup: filename keys (`filename`, `file_name`, `image`, `path`), label keys (`label`, `class`, `category`, `class_name`, `predicted_label`, `prediction`), confidence keys (`confidence`, `score`, `probability`, `prob`).
       - `parse_streaming(file_path, sample_lookup, dataset_id, source, batch_size=5000)` method that yields `pd.DataFrame` batches.
       - `sample_lookup` is a `dict[str, str]` mapping filename -> sample_id, built by the caller from the samples table.
       - Each row: `id=uuid4`, `dataset_id`, `sample_id` from lookup, `category_name=label`, sentinel bbox values (0.0 for all), `area=0.0`, `is_crowd=False`, `source=source`, `confidence=float(confidence)`, `metadata=None`.
       - Skip lines where filename has no match in `sample_lookup` (silently).
       - Use a helper `_get_field(record, keys)` like the GT parser does.

    2. Update `app/models/prediction.py`:
       - Add `"classification_jsonl"` to the `format` Literal type: `Literal["coco", "detection_annotation", "classification_jsonl"]`

    3. Update `app/routers/datasets.py` `import_predictions`:
       - Add an `elif request.format == "classification_jsonl":` branch after the existing `detection_annotation` branch.
       - Build `sample_lookup: dict[str, str]` from `SELECT id, file_name FROM samples WHERE dataset_id = ?`.
       - Instantiate `ClassificationPredictionParser()`.
       - Call `parse_streaming(prediction_path, sample_lookup, dataset_id, source=run_name)`.
       - Insert each batch with `INSERT INTO annotations SELECT * FROM batch_df`.
       - Track `total_inserted` and `total_skipped` (count lines in file minus inserted).
       - Validate prediction_path exists and is a file (not a directory); raise 400 if not.

    4. Update `app/routers/_run_name.py`:
       - Add `if fmt == "classification_jsonl": return _from_coco(prediction_path)` (use file stem as run name, same as COCO).
       - Add before the existing `if fmt == "detection_annotation":` check.

    Important: Do NOT modify the existing detection prediction parsers or evaluation services.
  </action>
  <verify>
    - `python -c "from app.ingestion.classification_prediction_parser import ClassificationPredictionParser; print('OK')"` succeeds
    - `python -c "from app.models.prediction import PredictionImportRequest; r = PredictionImportRequest(prediction_path='/tmp/test.jsonl', format='classification_jsonl'); print(r.format)"` prints `classification_jsonl`
    - App starts without import errors: `cd app && python -c "from app.routers.datasets import router; print('OK')"`
  </verify>
  <done>
    - ClassificationPredictionParser parses JSONL predictions with flexible keys and sentinel bbox values
    - PredictionImportRequest accepts "classification_jsonl" format
    - import_predictions endpoint routes classification_jsonl format to the new parser
    - Run name derivation handles classification_jsonl format
  </done>
</task>

<task type="auto">
  <name>Task 2: Classification evaluation and error analysis services with endpoint routing</name>
  <files>
    app/models/classification_evaluation.py
    app/services/classification_evaluation.py
    app/services/classification_error_analysis.py
    app/routers/statistics.py
  </files>
  <action>
    1. Create `app/models/classification_evaluation.py`:
       - `ClassificationPerClassMetrics(BaseModel)`: `class_name: str`, `precision: float`, `recall: float`, `f1: float`, `support: int`
       - `ClassificationEvaluationResponse(BaseModel)`: `accuracy: float`, `macro_f1: float`, `weighted_f1: float`, `per_class_metrics: list[ClassificationPerClassMetrics]`, `confusion_matrix: list[list[int]]`, `confusion_matrix_labels: list[str]`, `conf_threshold: float`, `evaluation_type: str = "classification"`

    2. Create `app/services/classification_evaluation.py`:
       - `compute_classification_evaluation(cursor, dataset_id, source, conf_threshold, split=None)` returning `ClassificationEvaluationResponse`.
       - Query GT and prediction labels per sample via JOIN:
         ```sql
         SELECT s.id, gt.category_name as gt_label, pred.category_name as pred_label
         FROM samples s
         JOIN annotations gt ON gt.sample_id = s.id AND gt.dataset_id = s.dataset_id AND gt.source = 'ground_truth'
         LEFT JOIN annotations pred ON pred.sample_id = s.id AND pred.dataset_id = s.dataset_id AND pred.source = ? AND pred.confidence >= ?
         WHERE s.dataset_id = ?
         ```
         (Add `AND s.split = ?` if split is provided.)
       - For multi-label GT: if a sample has multiple GT annotations, use the first one only (GROUP BY sample_id, take MIN(gt.category_name) or use DISTINCT ON equivalent).
       - Build confusion matrix as dict-of-dicts, derive labels from sorted unique classes (both GT and predicted).
       - Compute from confusion matrix: accuracy = trace/sum, per-class precision/recall/F1 with div-by-zero guards (return 0.0), macro F1 = mean of per-class F1, weighted F1 = sum(f1_i * support_i) / total_support.
       - Use numpy for confusion matrix construction if convenient, or pure Python dict counting (either is fine for the scale).

       - `get_classification_confusion_cell_samples(cursor, dataset_id, source, actual_class, predicted_class, conf_threshold, split=None)` returning `list[str]` of sample_ids.
       - Simple query: JOIN gt and pred annotations WHERE gt.category_name = actual_class AND pred.category_name = predicted_class (with optional split filter).

    3. Create `app/services/classification_error_analysis.py`:
       - `classify_errors(cursor, dataset_id, source, conf_threshold, split=None)` returning `ErrorAnalysisResponse` (reuse the existing model from `app/models/error_analysis.py`).
       - Query same GT+pred join as evaluation.
       - Categories: "correct" (gt == pred), "misclassified" (gt != pred and pred is not None), "missing_prediction" (pred is None).
       - Build `ErrorSummary` with: `true_positives` = correct count, `hard_false_positives` = 0 (not applicable), `label_errors` = misclassified count, `false_negatives` = missing_prediction count.
       - Build `per_class` list as `PerClassErrors` for each class: tp = correct for that class, label_error = misclassified for that class, fn = missing for that class, hard_fp = 0.
       - Build `samples_by_type` dict: keys "correct", "misclassified", "missing_prediction", values are lists of `ErrorSample` with sample_id, error_type, category_name (GT class), confidence.

    4. Update `app/routers/statistics.py`:
       - Import the new classification services and models.
       - In `get_evaluation`: after verifying dataset exists, fetch `dataset_type` from the datasets table (add it to the existing SELECT). If `dataset_type == "classification"`, call `compute_classification_evaluation(cursor, dataset_id, source, conf_threshold, split=split)` instead of the detection one. Update the response_model to `EvaluationResponse | ClassificationEvaluationResponse` or remove the response_model constraint and let FastAPI infer.
       - In `get_confusion_cell_samples_endpoint`: similarly check dataset_type. If classification, call `get_classification_confusion_cell_samples` (no iou_threshold needed). Return same `ConfusionCellSamplesResponse`.
       - In `get_error_analysis`: check dataset_type. If classification, call `classify_errors` from the classification module (not the detection one). No iou_threshold needed.
       - Important: The IoU threshold parameter is still accepted but ignored for classification datasets (backward compatible).
  </action>
  <verify>
    - `python -c "from app.services.classification_evaluation import compute_classification_evaluation; print('OK')"` succeeds
    - `python -c "from app.services.classification_error_analysis import classify_errors; print('OK')"` succeeds
    - `python -c "from app.models.classification_evaluation import ClassificationEvaluationResponse; print('OK')"` succeeds
    - App starts without import errors: `cd app && python -c "from app.routers.statistics import router; print('OK')"`
  </verify>
  <done>
    - Classification evaluation returns accuracy, macro F1, weighted F1, per-class P/R/F1, confusion matrix
    - Classification confusion cell samples returns sample IDs without IoU matching
    - Classification error analysis returns correct/misclassified/missing categorization
    - Existing endpoints route to classification services when dataset_type is "classification"
    - Detection evaluation is completely untouched
  </done>
</task>

</tasks>

<verification>
1. Import classification predictions: POST /datasets/{id}/predictions with format=classification_jsonl should insert annotation rows with sentinel bbox values
2. Evaluate classification: GET /datasets/{id}/evaluation (for a classification dataset) should return accuracy, F1, confusion_matrix, per_class_metrics with evaluation_type="classification"
3. Confusion cell drill-down: GET /datasets/{id}/confusion-cell-samples should return sample IDs for classification without IoU
4. Error analysis: GET /datasets/{id}/error-analysis (for classification) should return correct/misclassified/missing categories
5. Detection datasets: All existing detection evaluation endpoints should work exactly as before
</verification>

<success_criteria>
- Classification predictions importable via existing endpoint with new format option
- Evaluation endpoint returns classification-specific response for classification datasets
- Confusion cell samples work without IoU matching for classification
- Error analysis uses correct/misclassified/missing categories for classification
- Zero changes to detection evaluation code paths
</success_criteria>

<output>
After completion, create `.planning/phases/16-classification-evaluation/16-01-SUMMARY.md`
</output>
