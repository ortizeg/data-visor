---
phase: 16-classification-evaluation
plan: 02
type: execute
wave: 2
depends_on: ["16-01"]
files_modified:
  - frontend/src/types/evaluation.ts
  - frontend/src/types/prediction.ts
  - frontend/src/types/error-analysis.ts
  - frontend/src/hooks/use-evaluation.ts
  - frontend/src/hooks/use-confusion-cell.ts
  - frontend/src/components/detail/prediction-import-dialog.tsx
  - frontend/src/components/stats/stats-dashboard.tsx
  - frontend/src/components/stats/evaluation-panel.tsx
  - frontend/src/components/stats/error-analysis-panel.tsx
  - frontend/src/components/grid/grid-cell.tsx
autonomous: true

must_haves:
  truths:
    - "User can select 'Classification JSONL' format in the prediction import dialog and import predictions"
    - "User sees Evaluation and Error Analysis tabs for classification datasets (un-hidden from Phase 15)"
    - "User sees accuracy, macro F1, weighted F1 metric cards in the evaluation panel for classification"
    - "User sees confusion matrix with click-to-filter for classification"
    - "User sees per-class precision/recall/F1 table for classification"
    - "User sees correct/misclassified/missing error categories in error analysis for classification"
    - "User sees GT and predicted class badges on grid thumbnails for classification datasets with predictions"
  artifacts:
    - path: "frontend/src/types/evaluation.ts"
      provides: "ClassificationEvaluationResponse type with discriminant field"
    - path: "frontend/src/components/stats/evaluation-panel.tsx"
      provides: "Classification evaluation rendering with metric cards, confusion matrix, per-class table"
    - path: "frontend/src/components/stats/error-analysis-panel.tsx"
      provides: "Classification error analysis with correct/misclassified/missing categories"
    - path: "frontend/src/components/grid/grid-cell.tsx"
      provides: "Predicted class badge alongside GT badge for classification"
  key_links:
    - from: "frontend/src/components/stats/evaluation-panel.tsx"
      to: "/datasets/{id}/evaluation"
      via: "useEvaluation hook"
      pattern: "evaluation_type.*classification"
    - from: "frontend/src/components/stats/stats-dashboard.tsx"
      to: "evaluation-panel.tsx"
      via: "Evaluation tab now visible for classification datasets"
      pattern: "isClassification.*evaluation"
    - from: "frontend/src/components/grid/grid-cell.tsx"
      to: "annotations"
      via: "Finding prediction annotation to display predicted label"
      pattern: "source.*ground_truth.*prediction"
---

<objective>
Build frontend classification evaluation UI: prediction import format option, evaluation panel with metrics/confusion matrix/per-class table, error analysis panel, and GT vs predicted badges on grid.

Purpose: Give users a complete classification evaluation experience -- importing predictions, viewing metrics (accuracy, F1), exploring the confusion matrix, analyzing errors, and seeing GT vs predicted labels on thumbnails.

Output: Updated prediction import dialog, classification evaluation panel, classification error analysis, and grid cell predicted label badges.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-classification-evaluation/16-RESEARCH.md
@.planning/phases/16-classification-evaluation/16-01-SUMMARY.md
@.planning/phases/15-classification-ingestion-display/15-02-SUMMARY.md

Key existing files to reference:
@frontend/src/types/evaluation.ts              -- Add classification response type
@frontend/src/types/prediction.ts              -- Add classification_jsonl format
@frontend/src/hooks/use-evaluation.ts          -- Evaluation query hook
@frontend/src/hooks/use-confusion-cell.ts      -- Confusion cell click handler
@frontend/src/components/detail/prediction-import-dialog.tsx  -- Add format option
@frontend/src/components/stats/stats-dashboard.tsx  -- Un-hide tabs for classification
@frontend/src/components/stats/evaluation-panel.tsx -- Branch for classification
@frontend/src/components/stats/error-analysis-panel.tsx  -- Branch for classification
@frontend/src/components/stats/confusion-matrix.tsx      -- Reusable, no changes needed
@frontend/src/components/stats/metrics-cards.tsx         -- Reusable for metric cards
@frontend/src/components/stats/per-class-table.tsx       -- Detection version, reference
@frontend/src/components/grid/grid-cell.tsx              -- Add predicted label badge
</context>

<tasks>

<task type="auto">
  <name>Task 1: Types, hooks, prediction import dialog, and grid predicted label badge</name>
  <files>
    frontend/src/types/evaluation.ts
    frontend/src/types/prediction.ts
    frontend/src/hooks/use-evaluation.ts
    frontend/src/hooks/use-confusion-cell.ts
    frontend/src/components/detail/prediction-import-dialog.tsx
    frontend/src/components/grid/grid-cell.tsx
  </files>
  <action>
    1. Update `frontend/src/types/evaluation.ts`:
       - Add `ClassificationPerClassMetrics` interface: `class_name: string`, `precision: number`, `recall: number`, `f1: number`, `support: number`.
       - Add `ClassificationEvaluationResponse` interface: `accuracy: number`, `macro_f1: number`, `weighted_f1: number`, `per_class_metrics: ClassificationPerClassMetrics[]`, `confusion_matrix: number[][]`, `confusion_matrix_labels: string[]`, `conf_threshold: number`, `evaluation_type: "classification"`.
       - Add `evaluation_type?: "detection"` to the existing `EvaluationResponse` for type discrimination.
       - Export a union type: `export type AnyEvaluationResponse = EvaluationResponse | ClassificationEvaluationResponse;`

    2. Update `frontend/src/types/prediction.ts`:
       - Add `"classification_jsonl"` to the `format` union: `format: "coco" | "detection_annotation" | "classification_jsonl"`

    3. Update `frontend/src/hooks/use-evaluation.ts`:
       - Change the generic type from `EvaluationResponse` to `AnyEvaluationResponse` in the `apiFetch` call.
       - This allows the hook to return either detection or classification evaluation data transparently.

    4. Update `frontend/src/hooks/use-confusion-cell.ts`:
       - Check if the hook passes `iou_threshold` -- for classification, the backend ignores it, so no change needed. Just verify the function still works (it should, since the backend accepts but ignores iou_threshold for classification).

    5. Update `frontend/src/components/detail/prediction-import-dialog.tsx`:
       - Add `{ value: "classification_jsonl", label: "Classification JSONL" }` to `FORMAT_OPTIONS`.
       - Update the input label from "Prediction Directory" to dynamically change based on format: if `format === "classification_jsonl"` show "Prediction File", else show "Prediction Path" (classification JSONL is a single file, not a directory).

    6. Update `frontend/src/components/grid/grid-cell.tsx`:
       - In the classification branch (where `ClassBadge` is rendered), also find the first prediction annotation: `annotations.find(a => a.source !== "ground_truth")`.
       - If a prediction annotation exists, render a small badge at bottom-right of the thumbnail:
         - Green background (`bg-green-500/80 text-white`) if predicted label matches GT label.
         - Red background (`bg-red-500/80 text-white`) if predicted label differs from GT label.
         - Show the predicted class name in the badge.
         - Style: `absolute bottom-1 right-1 z-10 rounded px-1.5 py-0.5 text-[10px] font-semibold`.
  </action>
  <verify>
    - `cd frontend && npx tsc --noEmit` passes with no type errors
    - Prediction import dialog renders 3 format options (visual check via dev server)
  </verify>
  <done>
    - ClassificationEvaluationResponse type exists with all required fields
    - PredictionImportRequest accepts classification_jsonl format
    - useEvaluation hook returns AnyEvaluationResponse (works for both dataset types)
    - Prediction import dialog shows "Classification JSONL" format option
    - Grid cells show predicted class badge (green=correct, red=mismatch) alongside GT badge
  </done>
</task>

<task type="auto">
  <name>Task 2: Classification evaluation panel and error analysis panel</name>
  <files>
    frontend/src/components/stats/stats-dashboard.tsx
    frontend/src/components/stats/evaluation-panel.tsx
    frontend/src/components/stats/error-analysis-panel.tsx
  </files>
  <action>
    1. Update `frontend/src/components/stats/stats-dashboard.tsx`:
       - Change the Evaluation tab guard from `{!isClassification && (` to always show the Evaluation tab (remove the `!isClassification` wrapper). Keep the `disabled={!hasPredictions}` check.
       - Change the Error Analysis tab guard similarly: always show, keep `disabled={!hasPredictions}`.
       - Keep Worst Images and Intelligence tabs hidden for classification (`!isClassification`).
       - Pass `datasetType` prop to `EvaluationPanel` and `ErrorAnalysisPanel`:
         - `<EvaluationPanel datasetId={datasetId} split={split} excludedClasses={excludedClasses} datasetType={datasetType} />`
         - `<ErrorAnalysisPanel datasetId={datasetId} split={split} datasetType={datasetType} />`

    2. Update `frontend/src/components/stats/evaluation-panel.tsx`:
       - Accept `datasetType?: string` prop.
       - Add `const isClassification = datasetType === "classification";`
       - **Controls section:** Hide the IoU slider when `isClassification` (classification has no IoU). Keep the confidence slider and source dropdown visible.
       - **Data rendering:** After fetching evaluation data, check `evaluation_type`:
         - If `evaluation_type === "classification"` (or `isClassification`), render classification layout:
           - **Metric cards row (3 cards):** Accuracy, Macro F1, Weighted F1. Use the existing `MetricsCards` component pattern or render 3 simple stat cards inline: `<div className="grid grid-cols-3 gap-4">` with each card showing label + percentage value formatted to 1 decimal.
           - **Confusion matrix:** Reuse existing `<ConfusionMatrix>` component unchanged -- it takes `matrix` and `labels` props generically. Wire `onCellClick` the same way as detection (calls `fetchConfusionCellSamples` -> sets filter store sample IDs).
           - **Per-class table:** Render a table with columns: Class, Precision, Recall, F1, Support. Map over `per_class_metrics`. Use the same styling as the existing `PerClassTable` component. Can either reuse `PerClassTable` with a wrapper or render inline (the classification fields differ from detection: no AP columns).
         - If detection (no `evaluation_type` or `evaluation_type === "detection"`), render existing detection layout unchanged.

    3. Update `frontend/src/components/stats/error-analysis-panel.tsx`:
       - Accept `datasetType?: string` prop.
       - Add `const isClassification = datasetType === "classification";`
       - **Controls:** Hide IoU slider for classification. Keep confidence slider and source dropdown.
       - **Summary cards:** When classification, show 3 cards: Correct (green), Misclassified (red/amber), Missing Prediction (orange). Map from the error response: `true_positives` = correct count, `label_errors` = misclassified count, `false_negatives` = missing prediction count.
       - **Bar chart:** When classification, show bars for Correct/Misclassified/Missing instead of TP/Hard FP/Label Error/FN. Use same `Recharts` bar chart pattern.
       - **Sample grid:** When classification, show error samples grouped by type (correct, misclassified, missing_prediction) using same thumbnail grid pattern as detection error analysis.
       - For detection datasets, render everything exactly as before (no changes to existing behavior).
  </action>
  <verify>
    - `cd frontend && npx tsc --noEmit` passes with no type errors
    - `cd frontend && npm run build` succeeds
  </verify>
  <done>
    - Evaluation tab visible for classification datasets (un-hidden from Phase 15)
    - Error Analysis tab visible for classification datasets (un-hidden from Phase 15)
    - Worst Images and Intelligence tabs remain hidden for classification
    - Classification evaluation panel shows accuracy, macro F1, weighted F1 cards + confusion matrix + per-class table
    - IoU slider hidden for classification in both evaluation and error analysis panels
    - Classification error analysis shows correct/misclassified/missing categories
    - Detection evaluation and error analysis completely unchanged
  </done>
</task>

</tasks>

<verification>
1. Import predictions: User selects "Classification JSONL" in import dialog, browses to a .jsonl file, imports successfully
2. Evaluation tab: Shows for classification datasets with predictions; displays accuracy, F1, confusion matrix, per-class table
3. Confusion matrix click: Clicking a cell filters the grid to images with that GT/predicted pair
4. Error analysis: Shows correct/misclassified/missing categories with summary cards and sample grid
5. Grid badges: Classification thumbnails show GT badge (top) and predicted badge (bottom-right, green/red)
6. Detection regression: All detection evaluation/error analysis features work exactly as before
7. No IoU slider: Classification evaluation and error analysis panels hide the IoU threshold slider
</verification>

<success_criteria>
- Classification prediction import dialog works with new format option
- Evaluation panel renders classification-specific metrics (accuracy, F1, confusion matrix, per-class P/R/F1)
- Error analysis panel renders classification-specific categories (correct, misclassified, missing)
- Confusion matrix click-to-filter works for classification
- Grid shows GT + predicted badges for classification with color coding
- Detection dataset evaluation is completely unaffected
</success_criteria>

<output>
After completion, create `.planning/phases/16-classification-evaluation/16-02-SUMMARY.md`
</output>
