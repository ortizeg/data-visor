---
phase: 07-intelligence-agents
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - app/services/vlm_service.py
  - app/routers/vlm.py
  - app/main.py
  - app/dependencies.py
  - frontend/src/types/vlm.ts
  - frontend/src/hooks/use-vlm-progress.ts
  - frontend/src/components/toolbar/auto-tag-button.tsx
  - frontend/src/app/datasets/[datasetId]/page.tsx
autonomous: true

must_haves:
  truths:
    - "User can trigger VLM auto-tagging from the dataset page and monitor progress via SSE"
    - "Tags (dark, blurry, indoor, outdoor, crowded, etc.) appear in the existing samples.tags column after tagging completes"
    - "VLM model loads on-demand (not at startup) to avoid memory pressure with DINOv2"
    - "Tags are validated against a controlled vocabulary and invalid VLM responses are discarded"
  artifacts:
    - path: "app/services/vlm_service.py"
      provides: "Moondream2 loading via transformers, per-image tagging with encode-once optimization"
      contains: "class VLMService"
    - path: "app/routers/vlm.py"
      provides: "POST /auto-tag and GET /auto-tag/progress endpoints"
      contains: "auto_tag"
    - path: "frontend/src/components/toolbar/auto-tag-button.tsx"
      provides: "Auto-tag button with SSE progress indicator"
      contains: "AutoTagButton"
  key_links:
    - from: "app/routers/vlm.py"
      to: "app/services/vlm_service.py"
      via: "BackgroundTasks.add_task(vlm_service.generate_tags)"
      pattern: "generate_tags"
    - from: "app/services/vlm_service.py"
      to: "DuckDB samples.tags"
      via: "UPDATE samples SET tags = list_distinct(list_concat(tags, ?))"
      pattern: "list_concat"
    - from: "frontend/src/components/toolbar/auto-tag-button.tsx"
      to: "app/routers/vlm.py"
      via: "POST /auto-tag trigger + GET /auto-tag/progress SSE"
      pattern: "auto-tag"
---

<objective>
Implement the VLM auto-tagging pipeline: Moondream2 model loading, background batch tagging with SSE progress, and a frontend button to trigger and monitor auto-tagging. Tags are stored in the existing samples.tags VARCHAR[] column.

Purpose: Delivers AGENT-04 (VLM auto-tagging). Tags enrich sample metadata with attributes like "dark", "blurry", "indoor", "crowded" that the agent (Plan 01) can correlate with error patterns.
Output: Working auto-tag button on dataset page that triggers VLM tagging, shows progress, and stores validated tags in DuckDB.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-intelligence-agents/07-RESEARCH.md
@.planning/phases/07-intelligence-agents/07-01-SUMMARY.md

@app/services/embedding_service.py
@app/routers/embeddings.py
@app/routers/vlm.py
@app/config.py
@app/main.py
@app/dependencies.py
@frontend/src/hooks/use-embedding-progress.ts
@frontend/src/app/datasets/[datasetId]/page.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement VLM service and fill router endpoints</name>
  <files>
    app/services/vlm_service.py
    app/routers/vlm.py
    app/main.py
    app/dependencies.py
  </files>
  <action>
    1. Create `app/services/vlm_service.py` following the EmbeddingService pattern:
       - Import: `AutoModelForCausalLM` from `transformers`, `Image` from `PIL`, `torch`, `BytesIO` from `io`
       - Import: `DuckDBRepo` from `app.repositories.duckdb_repo`, `StorageBackend` from `app.repositories.storage`

       - Define TAG_PROMPTS dict (5 dimensions):
         ```python
         TAG_PROMPTS = {
             "lighting": "Describe the lighting: is this image dark, dim, bright, or normal? One word only.",
             "clarity": "Is this image blurry, sharp, or noisy? One word only.",
             "setting": "Is this scene indoor or outdoor? One word only.",
             "weather": "What weather or time: sunny, cloudy, rainy, foggy, snowy, night, or day? One word.",
             "density": "How crowded is this scene: empty, sparse, moderate, or crowded? One word only.",
         }
         ```

       - Define VALID_TAGS dict (controlled vocabulary per dimension):
         ```python
         VALID_TAGS = {
             "lighting": {"dark", "dim", "bright", "normal"},
             "clarity": {"blurry", "sharp", "noisy"},
             "setting": {"indoor", "outdoor"},
             "weather": {"sunny", "cloudy", "rainy", "foggy", "snowy", "night", "day"},
             "density": {"empty", "sparse", "moderate", "crowded"},
         }
         ```

       - Define `TaggingProgress` as a Pydantic BaseModel: status (str), processed (int), total (int), message (str = "")

       - Class `VLMService`:
         - `__init__(self, db: DuckDBRepo, storage: StorageBackend, device: str = "cpu")`: store db, storage, device. Init `_model = None`, `_tasks: dict[str, TaggingProgress] = {}`

         - `load_model(self)`: Load Moondream2 via transformers:
           ```python
           self._model = AutoModelForCausalLM.from_pretrained(
               "vikhyatk/moondream2",
               revision="2025-06-21",
               trust_remote_code=True,
               device_map={"": self._device},
           )
           ```
           Log the model load.

         - `_ensure_model(self)`: If `_model` is None, call `load_model()`. On-demand loading pattern.

         - `tag_image(self, image: Image.Image) -> list[str]`:
           Encode image ONCE with `encoded = self._model.encode_image(image)`, then run each TAG_PROMPT against the encoded image using `self._model.query(encoded, prompt)`. Parse the answer: `raw = result["answer"].strip().lower().rstrip(".")`. Validate against VALID_TAGS[dimension]. Only include tags that match the valid set. Return list of valid tags.

         - `get_progress(self, dataset_id: str) -> TaggingProgress`: Return current progress or idle default.

         - `is_running(self, dataset_id: str) -> bool`: Check if task status is "running".

         - `generate_tags(self, dataset_id: str) -> None`: Background task method (follows EmbeddingService.generate_embeddings pattern):
           1. Set `_tasks[dataset_id] = TaggingProgress(status="running", processed=0, total=0)`
           2. Call `_ensure_model()` to lazy-load
           3. Create cursor, count total samples for this dataset
           4. Process samples ONE AT A TIME (VLM inference is slow, not batchable like DINOv2):
              - Query samples in batches of 50 (for DB efficiency): `SELECT id, file_name FROM samples WHERE dataset_id = ? ORDER BY id LIMIT 50 OFFSET ?`
              - For each sample: load image via storage.read_bytes + Image.open + convert("RGB")
              - Call `tag_image(image)` to get tags
              - Merge tags into existing tags: `UPDATE samples SET tags = list_distinct(list_concat(tags, ?::VARCHAR[])) WHERE id = ? AND dataset_id = ?`
              - Update progress: `_tasks[dataset_id].processed += 1`
              - Skip samples with load errors (log warning, continue)
           5. On completion: status="complete", message=f"Tagged {total} samples"
           6. On error: status="error", message=str(e), log traceback
           7. Close cursor in finally block.

    2. Replace the stub `app/routers/vlm.py` (created by Plan 01) with full implementation:
       - `router = APIRouter(prefix="/datasets", tags=["vlm"])`

       - POST `/{dataset_id}/auto-tag` (status_code=202):
         - Depends: get_db, get_vlm_service, BackgroundTasks
         - Check `vlm_service.is_running(dataset_id)` -> 409 "Already running"
         - Verify dataset exists -> 404
         - `background_tasks.add_task(vlm_service.generate_tags, dataset_id)`
         - Return `{"dataset_id": dataset_id, "status": "started", "message": "Auto-tagging started"}`

       - GET `/{dataset_id}/auto-tag/progress`:
         - Depends: get_vlm_service
         - SSE endpoint following embeddings progress pattern:
           ```python
           async def event_generator():
               while True:
                   progress = vlm_service.get_progress(dataset_id)
                   yield {"event": "progress", "data": json.dumps(progress.model_dump())}
                   if progress.status in ("complete", "error"):
                       break
                   await asyncio.sleep(0.5)
           return EventSourceResponse(event_generator())
           ```

    3. Update `app/dependencies.py`:
       - Add import: `from app.services.vlm_service import VLMService`
       - Add `get_vlm_service(request: Request) -> VLMService` function returning `request.app.state.vlm_service`

    4. Update `app/main.py` lifespan:
       - After similarity_service creation, add:
         ```python
         vlm_service = VLMService(db=db, storage=storage, device=settings.vlm_device)
         app.state.vlm_service = vlm_service
         ```
       - NOTE: Do NOT call vlm_service.load_model() in lifespan. VLM loads on-demand to avoid startup memory pressure.
       - Ensure `from app.routers import ... vlm` import already exists from Plan 01. If not, add it.
       - Ensure `app.include_router(vlm.router)` already exists from Plan 01. If not, add it.
  </action>
  <verify>
    ```bash
    cd /Users/ortizeg/1Projects/⛹️‍♂️\ Next\ Play/code/data-visor
    python -c "from app.services.vlm_service import VLMService, TAG_PROMPTS, VALID_TAGS; print(f'VLM Service OK, {len(TAG_PROMPTS)} prompt dimensions')"
    python -c "from app.routers.vlm import router; print('VLM router OK:', [r.path for r in router.routes])"
    python -c "from app.main import app; print('App loads OK')"
    ```
  </verify>
  <done>
    - VLMService with on-demand model loading, encode-once tagging, and background task pipeline
    - POST /auto-tag returns 202 and triggers background tagging
    - GET /auto-tag/progress streams SSE progress events
    - Tags validated against controlled vocabulary before storage
    - Tags merged into existing samples.tags via list_distinct(list_concat(...))
  </done>
</task>

<task type="auto">
  <name>Task 2: Frontend auto-tag button with SSE progress</name>
  <files>
    frontend/src/types/vlm.ts
    frontend/src/hooks/use-vlm-progress.ts
    frontend/src/components/toolbar/auto-tag-button.tsx
    frontend/src/app/datasets/[datasetId]/page.tsx
  </files>
  <action>
    1. Create `frontend/src/types/vlm.ts`:
       ```typescript
       export interface TaggingProgress {
         status: "idle" | "running" | "complete" | "error";
         processed: number;
         total: number;
         message: string;
       }
       ```

    2. Create `frontend/src/hooks/use-vlm-progress.ts`:
       Reuse the same SSE pattern from `use-embedding-progress.ts` but typed for TaggingProgress:
       - `useVLMProgress(path: string, enabled: boolean): TaggingProgress`
       - Uses EventSource, listens for "progress" events, parses TaggingProgress JSON
       - Closes on "complete" or "error" status
       - Returns idle default when not enabled
       This is intentionally a separate hook (not generic) to keep the pattern simple and avoid over-abstraction.

    3. Create `frontend/src/components/toolbar/auto-tag-button.tsx`:
       NOTE: This file path was previously deleted (overlay-toggle.tsx was there). The toolbar directory should still exist.

       Component: `AutoTagButton({ datasetId }: { datasetId: string })`
       - State: `isTagging` (boolean) to track if we triggered tagging
       - `useMutation` from TanStack Query for POST `/datasets/${datasetId}/auto-tag`
       - `useVLMProgress(...)` for SSE progress, enabled when `isTagging` is true
       - When progress reaches "complete" or "error", reset `isTagging` to false and invalidate relevant queries (samples, filter-facets)

       Render:
       - When idle: Button with tag icon and "Auto-Tag" label. onClick triggers mutation and sets isTagging=true.
       - When running: Show progress bar or text: "Tagging... {processed}/{total}". Button disabled.
       - When complete: Briefly show "Done!" then reset to idle.
       - When error: Show error message briefly, reset to idle.

       Style: Use the same Tailwind patterns as existing toolbar buttons. Small, compact button that fits in the header area.

    4. Update `frontend/src/app/datasets/[datasetId]/page.tsx`:
       - Import `AutoTagButton` from `@/components/toolbar/auto-tag-button`
       - Add the `<AutoTagButton datasetId={datasetId} />` in the header area, after the tab switcher div, within the dataset info section. Place it as a utility action alongside the existing header controls.
  </action>
  <verify>
    ```bash
    cd /Users/ortizeg/1Projects/⛹️‍♂️\ Next\ Play/code/data-visor/frontend
    npx tsc --noEmit 2>&1 | head -30
    ```
    TypeScript compilation should succeed with no errors related to vlm types, hook, or auto-tag component.
  </verify>
  <done>
    - AutoTagButton component renders in dataset page header
    - Clicking "Auto-Tag" triggers POST /auto-tag and opens SSE progress stream
    - Progress shown as processed/total count during tagging
    - On completion, relevant query caches invalidated so tag facets and sample tags refresh
    - TypeScript compiles without errors
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.main import app; print('App OK')"` -- app loads without errors
2. Start backend, start frontend: Auto-Tag button visible in dataset page header
3. Click Auto-Tag: POST returns 202, SSE stream shows progress events
4. After completion: samples show new tags in their tags field
5. TypeScript compiles cleanly: `cd frontend && npx tsc --noEmit`
</verification>

<success_criteria>
- VLM service loads Moondream2 on-demand via transformers (not at startup)
- Background tagging processes all samples with SSE progress streaming
- Tags validated against controlled vocabulary (invalid responses discarded)
- Tags merged into existing samples.tags column (not overwritten)
- Frontend button triggers tagging, shows progress, and invalidates caches on completion
- No regression in existing features (additive changes only)
</success_criteria>

<output>
After completion, create `.planning/phases/07-intelligence-agents/07-02-SUMMARY.md`
</output>
