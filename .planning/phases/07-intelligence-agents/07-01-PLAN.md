---
phase: 07-intelligence-agents
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - app/config.py
  - app/models/agent.py
  - app/services/agent_service.py
  - app/routers/agent.py
  - app/routers/vlm.py
  - app/main.py
  - app/dependencies.py
autonomous: true

must_haves:
  truths:
    - "POST /datasets/{id}/analyze returns structured AnalysisReport with patterns and recommendations"
    - "Agent uses DuckDB tools to query error distributions and tag correlations (not raw LLM math)"
    - "Agent model is configurable via VISIONLENS_AGENT_MODEL env var"
    - "Missing API key returns clear error (not crash)"
  artifacts:
    - path: "app/models/agent.py"
      provides: "PatternInsight, Recommendation, AnalysisReport Pydantic models"
      contains: "class AnalysisReport"
    - path: "app/services/agent_service.py"
      provides: "Pydantic AI agent with DuckDB query tools"
      contains: "analysis_agent"
    - path: "app/routers/agent.py"
      provides: "POST /datasets/{id}/analyze endpoint"
      contains: "analyze_errors"
    - path: "app/routers/vlm.py"
      provides: "Stub router for VLM auto-tagging (filled by Plan 02)"
      contains: "router = APIRouter"
  key_links:
    - from: "app/routers/agent.py"
      to: "app/services/agent_service.py"
      via: "run_analysis() call"
      pattern: "run_analysis"
    - from: "app/services/agent_service.py"
      to: "app/services/error_analysis.py"
      via: "categorize_errors() for pre-computed error data"
      pattern: "categorize_errors"
    - from: "app/main.py"
      to: "app/routers/agent.py"
      via: "include_router"
      pattern: "agent\\.router"
---

<objective>
Set up the Pydantic AI agent backend for error pattern analysis with DuckDB query tools, structured output models, and the /analyze API endpoint. Also wire the VLM router stub and all infrastructure updates (config, main.py, dependencies) needed by downstream plans.

Purpose: Delivers AGENT-01 (error pattern detection) and AGENT-02 (action recommendations) backend. Creates the foundation that Plan 02 (VLM) and Plan 03 (frontend) extend.
Output: Working POST /analyze endpoint that runs an LLM agent with DuckDB tools and returns structured pattern insights and recommendations.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-intelligence-agents/07-RESEARCH.md

@app/config.py
@app/main.py
@app/dependencies.py
@app/services/error_analysis.py
@app/models/error_analysis.py
@app/routers/statistics.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install pydantic-ai-slim, create agent models and service</name>
  <files>
    pyproject.toml
    app/models/agent.py
    app/services/agent_service.py
  </files>
  <action>
    1. Install pydantic-ai-slim:
       ```bash
       cd /Users/ortizeg/1Projects/⛹️‍♂️\ Next\ Play/code/data-visor && uv add pydantic-ai-slim
       ```

    2. Create `app/models/agent.py` with these Pydantic models:
       - `PatternInsight`: pattern (str), evidence (str), severity (Literal["high", "medium", "low"]), affected_classes (list[str], default_factory=list)
       - `Recommendation`: action (str), rationale (str), priority (Literal["high", "medium", "low"]), category (Literal["data_collection", "augmentation", "labeling", "architecture", "hyperparameter"])
       - `AnalysisReport`: patterns (list[PatternInsight]), recommendations (list[Recommendation]), summary (str)
       - `AnalysisRequest`: source (str = "prediction"), iou_threshold (float = 0.5), conf_threshold (float = 0.25)
       All fields should have Field(description=...) for the LLM to understand the schema.

    3. Create `app/services/agent_service.py`:
       - Import from pydantic_ai: Agent, RunContext
       - Define `AnalysisDeps` dataclass with: cursor (DuckDBPyConnection), dataset_id (str), source (str), error_summary (dict)
       - Define the agent:
         ```python
         from app.config import get_settings
         settings = get_settings()
         analysis_agent = Agent(
             settings.agent_model,
             deps_type=AnalysisDeps,
             output_type=AnalysisReport,
             instructions="You are an ML engineer specializing in object detection error analysis. ..."
         )
         ```
         NOTE: Agent model string must be read at agent instantiation time. Since settings uses lru_cache, read it lazily. Use a function `_get_agent()` that creates and caches the agent on first call, or create the agent at module level but defer model resolution. Simplest: create a function `run_analysis()` that instantiates the agent inside.

       - Define 4 agent tools using @analysis_agent.tool decorator:
         a. `get_error_summary(ctx)` - Returns the pre-computed error summary from deps (str representation). No SQL needed.
         b. `get_per_class_errors(ctx)` - Queries annotations table: GROUP BY category_name, count by source='ground_truth' vs source=deps.source. Format as readable table string. LIMIT 50.
         c. `get_tag_error_correlation(ctx, error_type: str)` - Joins samples.tags with error samples from categorize_errors() output. Since error data is in-memory (not a table), pass error sample_ids through deps and use them in an IN clause. Return "No tag data available" if samples have no tags.
         d. `get_confidence_distribution(ctx, error_type: str)` - Uses error samples from deps, buckets confidence into ranges (0.0-0.3, 0.3-0.5, 0.5-0.7, 0.7-0.9, 0.9-1.0), returns counts per bucket.

       - Define `run_analysis(cursor, dataset_id, source, iou_threshold, conf_threshold) -> AnalysisReport` function:
         1. Call `categorize_errors()` to get error data
         2. Build AnalysisDeps with cursor, dataset_id, source, error_summary as dict
         3. Store error samples in deps for tool access (e.g., deps.error_samples = error_data.samples_by_type)
         4. Call `analysis_agent.run_sync(prompt, deps=deps)`
         5. Return result.output
         6. Wrap in try/except: if agent model not configured or API key missing, raise a clear ValueError

       - Define a helper `_format_table(headers, rows)` that formats query results as a pipe-separated table string for LLM readability.

       IMPORTANT: The agent needs the error samples data for tool queries (tag correlation, confidence distribution). Since error_samples are in-memory (from categorize_errors()), extend AnalysisDeps to include `error_samples: dict[str, list]` containing sample_ids grouped by error_type. Tools can then query DuckDB using these sample_ids.
  </action>
  <verify>
    ```bash
    cd /Users/ortizeg/1Projects/⛹️‍♂️\ Next\ Play/code/data-visor
    python -c "from app.models.agent import AnalysisReport, PatternInsight, Recommendation; print('Models OK')"
    python -c "from app.services.agent_service import run_analysis; print('Service OK')"
    ```
  </verify>
  <done>
    - pydantic-ai-slim installed in pyproject.toml
    - AnalysisReport, PatternInsight, Recommendation models defined with Field descriptions
    - Agent service with 4 DuckDB tools and run_analysis() entry point
    - Agent model configurable (not hardcoded)
  </done>
</task>

<task type="auto">
  <name>Task 2: Create routers, update config, wire into app</name>
  <files>
    app/config.py
    app/routers/agent.py
    app/routers/vlm.py
    app/main.py
    app/dependencies.py
  </files>
  <action>
    1. Update `app/config.py` -- add to Settings class:
       - `agent_model: str = "openai:gpt-4o"` -- Pydantic AI model string (e.g., "openai:gpt-4o", "anthropic:claude-sonnet-4-5")
       - `vlm_device: str = "cpu"` -- Device for Moondream2 VLM inference (cpu, mps, cuda). Default to cpu to avoid MPS memory pressure with DINOv2.

    2. Create `app/routers/agent.py`:
       - `router = APIRouter(prefix="/datasets", tags=["agent"])`
       - POST `/{dataset_id}/analyze` endpoint:
         - Parameters: dataset_id (path), body: AnalysisRequest (or individual Query params: source, iou_threshold, conf_threshold)
         - Use `get_db` dependency to get DuckDBRepo
         - Create cursor, call `run_analysis()` from agent_service
         - Return AnalysisReport
         - Handle ValueError (no API key) -> 503 with clear message "Configure VISIONLENS_AGENT_MODEL and corresponding API key"
         - Handle generic Exception -> 500 with error message

    3. Create `app/routers/vlm.py` (stub for Plan 02):
       - `router = APIRouter(prefix="/datasets", tags=["vlm"])`
       - POST `/{dataset_id}/auto-tag` endpoint stub that raises HTTPException(501, "VLM auto-tagging not yet implemented")
       - GET `/{dataset_id}/auto-tag/progress` endpoint stub that returns {"status": "idle", "processed": 0, "total": 0}
       This stub ensures main.py wiring is complete for Plan 02 to fill in.

    4. Update `app/main.py`:
       - Add imports: `from app.routers import agent, vlm` (in the existing noqa import line)
       - Add `app.include_router(agent.router)` and `app.include_router(vlm.router)` after existing router includes
       - No lifespan changes needed (agent is stateless, VLM loads on-demand in Plan 02)

    5. No changes needed to `app/dependencies.py` for this plan (agent service is stateless, called directly in router). VLM service dependency will be added in Plan 02.
  </action>
  <verify>
    ```bash
    cd /Users/ortizeg/1Projects/⛹️‍♂️\ Next\ Play/code/data-visor
    python -c "from app.routers.agent import router; print('Agent router OK:', [r.path for r in router.routes])"
    python -c "from app.routers.vlm import router; print('VLM router OK:', [r.path for r in router.routes])"
    python -c "from app.main import app; routes = [r.path for r in app.routes]; assert '/datasets/{dataset_id}/analyze' in routes, f'Missing analyze route in {routes}'; print('Main wiring OK')"
    ```
  </verify>
  <done>
    - Config has agent_model and vlm_device settings with VISIONLENS_ prefix
    - Agent router with POST /analyze endpoint that calls run_analysis and handles errors
    - VLM router stub with 501 placeholder (ready for Plan 02)
    - Both routers wired into main.py
    - curl POST /datasets/{id}/analyze returns 503 (no API key) or AnalysisReport (with key)
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.main import app; print('App loads OK')"` -- app starts without errors
2. `python -c "from app.models.agent import AnalysisReport; print(AnalysisReport.model_json_schema())"` -- schema matches spec
3. Start server and test: `curl -X POST http://localhost:8000/datasets/test/analyze` should return 404 (dataset not found) or 503 (no API key), not 500
4. `curl http://localhost:8000/datasets/test/auto-tag/progress` should return idle status
</verification>

<success_criteria>
- pydantic-ai-slim is installed and importable
- POST /datasets/{id}/analyze endpoint exists and returns structured AnalysisReport (or clear error if no API key)
- Agent has 4 DuckDB tools for error analysis queries
- Agent model is configurable via VISIONLENS_AGENT_MODEL env var
- VLM router stub is wired and returns 501 placeholder
- No changes needed to existing tests (additive only)
</success_criteria>

<output>
After completion, create `.planning/phases/07-intelligence-agents/07-01-SUMMARY.md`
</output>
