---
phase: 09-smart-ingestion
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/models/scan.py
  - app/services/folder_scanner.py
  - app/routers/ingestion.py
  - app/ingestion/coco_parser.py
  - app/services/ingestion.py
  - app/models/dataset.py
  - app/main.py
autonomous: true

must_haves:
  truths:
    - "POST /ingestion/scan accepts a folder path and returns detected COCO splits"
    - "FolderScanner detects standard COCO, Roboflow, and flat layouts"
    - "Ingestion service accepts an optional split parameter that populates the samples.split column"
    - "Multi-split import ingests all splits sequentially under one dataset_id"
  artifacts:
    - path: "app/models/scan.py"
      provides: "ScanRequest, ScanResult, DetectedSplit Pydantic models"
      contains: "class ScanResult"
    - path: "app/services/folder_scanner.py"
      provides: "FolderScanner with heuristic COCO detection"
      contains: "class FolderScanner"
    - path: "app/routers/ingestion.py"
      provides: "POST /ingestion/scan and POST /ingestion/import endpoints"
      contains: "router = APIRouter"
    - path: "app/ingestion/coco_parser.py"
      provides: "build_image_batches with optional split parameter"
      contains: "split"
    - path: "app/services/ingestion.py"
      provides: "ingest_with_progress with split param + ingest_splits_with_progress"
      contains: "ingest_splits_with_progress"
  key_links:
    - from: "app/routers/ingestion.py"
      to: "app/services/folder_scanner.py"
      via: "scan endpoint calls FolderScanner.scan()"
      pattern: "FolderScanner"
    - from: "app/routers/ingestion.py"
      to: "app/services/ingestion.py"
      via: "import endpoint calls ingest_splits_with_progress()"
      pattern: "ingest_splits_with_progress"
    - from: "app/services/ingestion.py"
      to: "app/ingestion/coco_parser.py"
      via: "split param passed through to build_image_batches()"
      pattern: "split="
    - from: "app/main.py"
      to: "app/routers/ingestion.py"
      via: "router registered with app.include_router()"
      pattern: "ingestion.router"
---

<objective>
Build the backend folder scanning service, scan/import API endpoints, and extend the existing ingestion pipeline to support split-aware multi-split imports.

Purpose: This is the backend foundation for smart ingestion. Users need an API that can scan a folder for COCO datasets, return the detected structure, and import multiple splits as a single dataset -- all coordinated through new endpoints that the frontend wizard will consume.
Output: FolderScanner service, Pydantic scan models, ingestion router with scan + import endpoints, split-aware ingestion pipeline
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-smart-ingestion/09-RESEARCH.md

@app/services/ingestion.py
@app/ingestion/coco_parser.py
@app/models/dataset.py
@app/routers/datasets.py
@app/main.py
@app/dependencies.py
@app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create FolderScanner service and Pydantic scan models</name>
  <files>app/models/scan.py, app/services/folder_scanner.py</files>
  <action>
**Create `app/models/scan.py`** with these Pydantic models:

```python
class DetectedSplit(BaseModel):
    name: str                    # Canonical split name: "train", "val", or "test"
    annotation_path: str         # Absolute path to COCO JSON annotation file
    image_dir: str               # Absolute path to image directory
    image_count: int             # Number of image files found in image_dir
    annotation_file_size: int    # Size in bytes of the annotation file

class ScanRequest(BaseModel):
    root_path: str               # Folder path to scan

class ScanResult(BaseModel):
    root_path: str               # Resolved absolute path
    dataset_name: str            # Inferred from root directory name
    format: str                  # "coco" (only supported format currently)
    splits: list[DetectedSplit]  # Detected importable splits
    warnings: list[str]          # Non-fatal issues (e.g., "Found JSON but not valid COCO")

class ImportRequest(BaseModel):
    dataset_name: str
    splits: list[ImportSplit]    # Splits to import (user may have deselected some)

class ImportSplit(BaseModel):
    name: str                    # "train", "val", "test", or custom
    annotation_path: str
    image_dir: str
```

**Create `app/services/folder_scanner.py`** with these specifications:

The `FolderScanner` class implements heuristic-based COCO dataset detection. It checks three layouts in priority order:

**Layout B (Roboflow -- most specific, check first):**
Split-named subdirectories (train/, val/, valid/, test/) where EACH subdirectory contains both a COCO annotation JSON file and image files co-located.
Detection: For each subdir matching SPLIT_DIR_NAMES, look for `.json` files inside it, check if any is COCO format (via ijson peek), and count images in the same directory.

**Layout A (Standard COCO):**
An `annotations/` directory at root containing per-split COCO JSON files, paired with either:
- An `images/` directory with split subdirectories (images/train2017/, images/val2017/), OR
- Split-named directories at root containing images (train2017/, val2017/ alongside annotations/)
Detection: Find `annotations/` dir, peek JSON files inside it for COCO format, match annotation filenames containing split keywords to image directories containing the same split keyword.

**Layout C (Flat -- fallback):**
A single COCO annotation JSON file at or near root, paired with an `images/` directory or image files in the same directory.
Detection: Scan root for `.json` files, peek for COCO format. Look for `images/` subdirectory or count images at root. Present as a single split named after the root directory.

**Helper methods:**

`is_coco_annotation(file_path: Path) -> bool`: Use `ijson.parse()` to read top-level keys. Return True if `"images"` key is found among the first 10 top-level keys. Wrap in try/except for `ijson.IncompleteJSONError` and `OSError`. Only check files < 500MB to avoid scanning enormous files during detection.

`count_images(dir_path: Path) -> int`: Use `os.scandir()` to count files with extensions in `{".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".webp"}` (case-insensitive). Do NOT recurse into subdirectories.

`detect_split_dirs(root: Path) -> dict[str, Path]`: Map canonical split names to directory paths. Use this lookup:
```python
SPLIT_DIR_NAMES = {
    "train": "train", "train2017": "train", "train2014": "train", "training": "train",
    "val": "val", "val2017": "val", "val2014": "val", "valid": "val", "validation": "val",
    "test": "test", "test2017": "test", "test2014": "test", "testing": "test",
}
```

The main `scan(root_path: str) -> ScanResult` method:
1. Resolve path to absolute via `Path(root_path).resolve()`
2. Validate it is a directory (raise ValueError if not)
3. Try Layout B, then Layout A, then Layout C
4. Return ScanResult with all detected splits (may be empty if nothing detected)
5. Add warnings for JSON files that failed COCO detection

Do NOT parse the entire annotation file -- only peek at top-level keys with ijson.
Do NOT recurse deeper than 2 levels from root.
Do NOT use `glob` for image counting -- use `os.scandir`.
  </action>
  <verify>
Run: `cd "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/data-visor" && python -c "from app.models.scan import ScanRequest, ScanResult, DetectedSplit, ImportRequest, ImportSplit; print('Models OK')"` to confirm models import.
Run: `cd "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/data-visor" && python -c "from app.services.folder_scanner import FolderScanner; print('Scanner OK')"` to confirm scanner imports.
Run existing tests: `cd "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/data-visor" && pytest tests/ -x -q` to confirm no regressions.
  </verify>
  <done>
app/models/scan.py contains ScanRequest, ScanResult, DetectedSplit, ImportRequest, and ImportSplit Pydantic models. app/services/folder_scanner.py contains FolderScanner class with scan() method supporting all three COCO layouts (Roboflow, Standard, Flat), using ijson peek for format detection and os.scandir for image counting. All existing tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ingestion router, extend ingestion pipeline with split support, and register router</name>
  <files>app/routers/ingestion.py, app/ingestion/coco_parser.py, app/services/ingestion.py, app/models/dataset.py, app/main.py</files>
  <action>
**Create `app/routers/ingestion.py`** with two endpoints:

1. `POST /ingestion/scan` -- Accepts `ScanRequest`, returns `ScanResult`:
   - Instantiate `FolderScanner` and call `scanner.scan(request.root_path)`
   - Catch `ValueError` and return 400 with the error message
   - If no splits detected, return 404 with "No COCO datasets detected in this directory"
   - If the path does not exist, return 400 with a helpful message: "Directory not found. If running in Docker, ensure the directory is volume-mounted."

2. `POST /ingestion/import` -- Accepts `ImportRequest`, returns SSE `StreamingResponse`:
   - Use FastAPI `Depends(get_ingestion_service)` to get the IngestionService
   - Call `ingestion_service.ingest_splits_with_progress(splits=request.splits, dataset_name=request.dataset_name)`
   - Wrap the generator in a `StreamingResponse` with `media_type="text/event-stream"` and the same headers as the existing ingest endpoint (Cache-Control, Connection, X-Accel-Buffering)
   - Each yielded `IngestionProgress` is serialized as `data: {json}\n\n`
   - The SSE event JSON includes an additional `split` field (which split is currently being imported)
   - Follow the EXACT same pattern as `datasets.py:37-73` (the existing ingest endpoint)

Router setup: `router = APIRouter(prefix="/ingestion", tags=["ingestion"])`

**Modify `app/ingestion/coco_parser.py`:**
Add an optional `split` parameter to `build_image_batches()`:

Change signature from:
```python
def build_image_batches(self, file_path: Path, dataset_id: str) -> Iterator[pd.DataFrame]:
```
To:
```python
def build_image_batches(self, file_path: Path, dataset_id: str, split: str | None = None) -> Iterator[pd.DataFrame]:
```

In the batch.append() dict, change `"split": None` to `"split": split`.

This is a 2-line change. Do NOT change any other behavior.

**Modify `app/services/ingestion.py`:**

1. Add `split: str | None = None` parameter to `ingest_with_progress()`:
   - Pass it through to `parser.build_image_batches(Path(annotation_path), dataset_id, split=split)`
   - No other changes to this method

2. Add a new method `ingest_splits_with_progress()`:
```python
def ingest_splits_with_progress(
    self,
    splits: list,   # List of ImportSplit-like objects with name, annotation_path, image_dir
    dataset_name: str,
) -> Iterator[IngestionProgress]:
    """Ingest multiple splits as a single dataset, yielding per-split progress."""
    for i, split_config in enumerate(splits):
        yield IngestionProgress(
            stage="split_start",
            current=i + 1,
            total=len(splits),
            message=f"Starting split: {split_config.name} ({i + 1}/{len(splits)})",
        )
        # Delegate to existing single-split ingestion
        # For first split, use dataset_name. For subsequent splits, reuse same dataset.
        yield from self.ingest_with_progress(
            annotation_path=split_config.annotation_path,
            image_dir=split_config.image_dir,
            dataset_name=dataset_name,
            split=split_config.name,
        )
```

IMPORTANT: The existing `ingest_with_progress` creates a new dataset_id for every call. For multi-split, all splits must share the SAME dataset_id. Refactor: extract dataset_id generation to be optionally passed in, OR generate it once in `ingest_splits_with_progress` and pass it down. The cleanest approach: add an optional `dataset_id: str | None = None` parameter to `ingest_with_progress()`. If None, generate a new UUID. If provided, reuse it. Then `ingest_splits_with_progress` generates one UUID and passes it to all calls.

Also, for subsequent splits after the first, the dataset record INSERT will fail because the dataset_id already exists. Handle this: only insert the dataset record on the FIRST split (or use INSERT OR REPLACE, or skip if exists). The simplest approach: add a `_dataset_exists` check before the INSERT INTO datasets, and UPDATE counts instead of INSERT for subsequent splits.

**Modify `app/models/dataset.py`:**
Add `split: str | None = None` to the `IngestRequest` model so the existing single-file ingest endpoint also supports split tagging.

**Modify `app/main.py`:**
Add the ingestion router registration:
```python
from app.routers import agent, datasets, embeddings, images, ingestion, samples, similarity, statistics, views, vlm
...
app.include_router(ingestion.router)
```
  </action>
  <verify>
Run: `cd "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/data-visor" && python -c "from app.routers.ingestion import router; print(f'Routes: {[r.path for r in router.routes]}')"` to confirm router has /scan and /import routes.
Run: `cd "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/data-visor" && python -c "from app.main import app; routes = [r.path for r in app.routes]; assert '/ingestion/scan' in routes; assert '/ingestion/import' in routes; print('Router registered OK')"` to confirm router is registered.
Run: `cd "/Users/ortizeg/1Projects/⛹️‍♂️ Next Play/code/data-visor" && pytest tests/ -x -q` to confirm all existing tests still pass.
Test the scan endpoint manually if a COCO dataset directory is available:
`curl -s -X POST http://localhost:8000/ingestion/scan -H "Content-Type: application/json" -d '{"root_path": "/path/to/coco/dataset"}' | python -m json.tool`
  </verify>
  <done>
app/routers/ingestion.py exists with POST /ingestion/scan and POST /ingestion/import endpoints. COCOParser.build_image_batches accepts optional split parameter. IngestionService.ingest_with_progress accepts optional split and dataset_id parameters. IngestionService.ingest_splits_with_progress imports multiple splits under one dataset_id. IngestRequest model includes optional split field. Ingestion router is registered in main.py. All existing tests pass.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.models.scan import ScanRequest, ScanResult, DetectedSplit, ImportRequest, ImportSplit; print('OK')"` succeeds
2. `python -c "from app.services.folder_scanner import FolderScanner; s = FolderScanner(); print(type(s))"` succeeds
3. `python -c "from app.routers.ingestion import router; print([r.path for r in router.routes])"` shows /scan and /import
4. `python -c "from app.main import app; print([r.path for r in app.routes if 'ingestion' in r.path])"` shows registered routes
5. `grep -q 'split' app/ingestion/coco_parser.py` confirms split parameter added
6. `grep -q 'ingest_splits_with_progress' app/services/ingestion.py` confirms multi-split method exists
7. `pytest tests/ -x -q` passes with no regressions
</verification>

<success_criteria>
- FolderScanner detects all three COCO layouts (Roboflow, Standard COCO, Flat) using ijson peek
- POST /ingestion/scan returns detected splits with image counts and annotation file sizes
- POST /ingestion/import streams SSE progress for multi-split sequential import
- Split parameter flows from API through IngestionService to COCOParser to DuckDB samples.split column
- All existing tests pass (no regressions from COCOParser and IngestionService modifications)
</success_criteria>

<output>
After completion, create `.planning/phases/09-smart-ingestion/09-01-SUMMARY.md`
</output>
