---
phase: 04-predictions-comparison
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/ingestion/prediction_parser.py
  - app/models/prediction.py
  - app/routers/datasets.py
  - app/repositories/duckdb_repo.py
  - tests/test_prediction_import.py
  - tests/fixtures/coco_predictions.json
autonomous: true

must_haves:
  truths:
    - "User can POST a COCO results JSON path and have predictions stored in the annotations table with source='prediction'"
    - "Existing ground truth annotations are never touched during prediction import"
    - "Re-importing predictions replaces previous predictions (not duplicates)"
    - "Dataset prediction_count column reflects the number of imported predictions"
  artifacts:
    - path: "app/ingestion/prediction_parser.py"
      provides: "Streaming COCO results parser yielding DataFrame batches"
      contains: "ijson.items"
    - path: "app/models/prediction.py"
      provides: "PredictionImportRequest and PredictionImportResponse Pydantic models"
      exports: ["PredictionImportRequest", "PredictionImportResponse"]
    - path: "app/routers/datasets.py"
      provides: "POST /datasets/{dataset_id}/predictions endpoint"
      contains: "import_predictions"
    - path: "tests/test_prediction_import.py"
      provides: "Integration test for prediction import flow"
      min_lines: 40
  key_links:
    - from: "app/routers/datasets.py"
      to: "app/ingestion/prediction_parser.py"
      via: "PredictionParser.parse_streaming()"
      pattern: "PredictionParser"
    - from: "app/ingestion/prediction_parser.py"
      to: "annotations table"
      via: "DataFrame bulk insert with source='prediction'"
      pattern: "source.*prediction"
    - from: "app/routers/datasets.py"
      to: "categories table"
      via: "Load category mapping to resolve category_id -> category_name"
      pattern: "categories.*dataset_id"
---

<objective>
Build the prediction import pipeline so users can import pre-computed model predictions from a COCO detection results JSON file and have them stored alongside ground truth annotations.

Purpose: This is the foundation of Phase 4 -- predictions must exist in the database before the comparison toggle or statistics dashboard can show anything meaningful.

Output: A streaming prediction parser, a POST endpoint for prediction import, updated dataset schema with prediction_count, and integration tests.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-predictions-comparison/04-RESEARCH.md

Source files to reference (read before implementing):
@app/ingestion/coco_parser.py -- Streaming ijson pattern to replicate
@app/ingestion/base_parser.py -- BaseParser interface
@app/repositories/duckdb_repo.py -- Schema and annotations table
@app/routers/datasets.py -- Existing endpoint patterns, where to add import_predictions
@app/services/ingestion.py -- SSE progress pattern to replicate
@app/models/dataset.py -- DatasetResponse model (needs prediction_count)
@app/dependencies.py -- DI pattern for composing services
</context>

<tasks>

<task type="auto">
  <name>Task 1: Prediction parser and Pydantic models</name>
  <files>
    app/ingestion/prediction_parser.py
    app/models/prediction.py
    app/repositories/duckdb_repo.py
    app/models/dataset.py
    tests/fixtures/coco_predictions.json
  </files>
  <action>
    **1. Create `app/models/prediction.py`:**
    - `PredictionImportRequest(BaseModel)`: `prediction_path: str` (file path to COCO results JSON on server filesystem)
    - `PredictionImportResponse(BaseModel)`: `dataset_id: str`, `prediction_count: int`, `skipped_count: int`, `message: str`

    **2. Create `app/ingestion/prediction_parser.py`:**
    - Class `PredictionParser` with method `parse_streaming(file_path: Path, category_map: dict[int, str], dataset_id: str, batch_size: int = 5000) -> Iterator[pd.DataFrame]`
    - Use `ijson.items(f, "item", use_float=True)` to stream-parse the flat COCO results array (same binary-mode pattern as coco_parser.py)
    - For each prediction dict: extract `image_id`, `category_id`, `bbox` (list of 4 floats), `score`
    - Resolve `category_id` to `category_name` using the provided `category_map`. Log warning and count skipped predictions for unmapped category_ids.
    - Convert `image_id` with `str(int(pred["image_id"]))` to match the DuckDB samples.id format (COCO parser uses same conversion)
    - Generate UUID for each annotation id: `str(uuid.uuid4())`
    - Build rows as dicts with columns matching annotations table order: `id, dataset_id, sample_id, category_name, bbox_x, bbox_y, bbox_w, bbox_h, area, is_crowd, source, confidence, metadata`
    - Set `source="prediction"`, `confidence=float(pred["score"])`, `is_crowd=False`, `area=bbox_w*bbox_h`, `metadata=None`
    - Yield `pd.DataFrame(batch)` every `batch_size` rows (same batching pattern as coco_parser.py)

    **3. Update `app/repositories/duckdb_repo.py`:**
    - Add `prediction_count INTEGER DEFAULT 0` column to the datasets table CREATE statement
    - This column tracks imported prediction count separately from annotation_count (which stays GT-only)

    **4. Update `app/models/dataset.py`:**
    - Add `prediction_count: int = 0` field to `DatasetResponse`
    - Update the `list_datasets` and `get_dataset` SELECT queries in `app/routers/datasets.py` to include `prediction_count` in the column list and the DatasetResponse constructor

    **5. Create `tests/fixtures/coco_predictions.json`:**
    - A small COCO results array with 5-10 prediction objects:
      ```json
      [
        {"image_id": 1, "category_id": 1, "bbox": [10.5, 20.3, 100.0, 80.0], "score": 0.95},
        {"image_id": 1, "category_id": 2, "bbox": [200.0, 150.0, 50.0, 60.0], "score": 0.82}
      ]
      ```
    - Include predictions for at least 2 different image_ids and 2 different category_ids
    - Include one prediction with an unmapped category_id (e.g., 999) to test the skip/warning path
  </action>
  <verify>
    - `python -c "from app.ingestion.prediction_parser import PredictionParser; print('OK')"` succeeds
    - `python -c "from app.models.prediction import PredictionImportRequest, PredictionImportResponse; print('OK')"` succeeds
    - `python -c "from app.models.dataset import DatasetResponse; print(DatasetResponse.model_fields.keys())"` shows `prediction_count`
  </verify>
  <done>
    PredictionParser streams COCO results JSON via ijson and yields DataFrame batches with correct column order matching the annotations table. PredictionImportRequest/Response models exist. datasets table has prediction_count column. Test fixture exists.
  </done>
</task>

<task type="auto">
  <name>Task 2: Prediction import endpoint and integration test</name>
  <files>
    app/routers/datasets.py
    app/dependencies.py
    tests/test_prediction_import.py
  </files>
  <action>
    **1. Add import endpoint to `app/routers/datasets.py`:**
    - `POST /datasets/{dataset_id}/predictions` accepting `PredictionImportRequest` body
    - Inject `db: DuckDBRepo = Depends(get_db)`
    - Flow:
      1. Verify dataset exists (404 if not)
      2. Load category mapping from categories table: `SELECT category_id, name FROM categories WHERE dataset_id = ?` -> build `dict[int, str]`
      3. DELETE existing predictions: `DELETE FROM annotations WHERE dataset_id = ? AND source = 'prediction'` (never touch ground_truth)
      4. Create `PredictionParser()` and call `parse_streaming(file_path, category_map, dataset_id)`
      5. For each DataFrame batch: `INSERT INTO annotations SELECT * FROM df` (same pattern as ingestion.py)
      6. Count total inserted and skipped predictions
      7. Update datasets table: `UPDATE datasets SET prediction_count = ? WHERE id = ?`
      8. Return `PredictionImportResponse` with counts and message
    - Do NOT use SSE/streaming for this endpoint -- it's simpler than full ingestion and a direct JSON response is sufficient. The prediction file is smaller than the annotation file (no images to process).
    - Handle `FileNotFoundError` by raising HTTPException(400, "Prediction file not found: {path}")

    **2. Update DatasetResponse column fetch:**
    - In `list_datasets()` and `get_dataset()` functions in datasets.py, add `prediction_count` to the SELECT column list and DatasetResponse constructor call (it was the 10th column before, now it's... make sure ordering is correct).

    **3. Create `tests/test_prediction_import.py`:**
    - Use the existing test infrastructure pattern (see tests/conftest.py)
    - Test 1: `test_import_predictions_success` -- Ingest a small COCO dataset first, then POST predictions, verify response has correct prediction_count, verify annotations table has predictions with source='prediction' and confidence values
    - Test 2: `test_import_predictions_replaces_existing` -- Import predictions twice, verify count doesn't double (old predictions deleted before new ones inserted)
    - Test 3: `test_import_predictions_preserves_ground_truth` -- Verify GT annotation count unchanged after prediction import
    - Test 4: `test_import_predictions_unknown_dataset` -- POST to nonexistent dataset_id returns 404
  </action>
  <verify>
    Run: `cd /Users/ortizeg/1Projects/⛹️‍♂️\ Next\ Play/code/data-visor && python -m pytest tests/test_prediction_import.py -v`
    All 4 tests pass.

    Manual check: `python -m pytest tests/ -v` -- all existing tests still pass (no regressions).
  </verify>
  <done>
    POST /datasets/{dataset_id}/predictions endpoint accepts a COCO results JSON path, stream-parses it, inserts predictions into annotations table with source='prediction', deletes previous predictions before re-import, updates prediction_count on the dataset, and returns a summary response. Integration tests verify the full flow including replace semantics and GT preservation.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/ -v` -- all tests pass including new prediction import tests
2. `python -c "from app.ingestion.prediction_parser import PredictionParser"` -- import succeeds
3. `python -c "from app.models.prediction import PredictionImportRequest"` -- import succeeds
4. Verify DuckDB schema: `python -c "from app.repositories.duckdb_repo import DuckDBRepo; db = DuckDBRepo('/tmp/test.db'); db.initialize_schema(); print(db.connection.execute('PRAGMA table_info(datasets)').fetchall())"` shows prediction_count column
</verification>

<success_criteria>
- Prediction import endpoint accepts COCO results JSON and stores predictions in annotations table
- Predictions have source='prediction' and confidence values from the score field
- Re-import replaces previous predictions (no duplicates), ground truth untouched
- Dataset prediction_count is updated after import
- All new and existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-predictions-comparison/04-01-SUMMARY.md`
</output>
