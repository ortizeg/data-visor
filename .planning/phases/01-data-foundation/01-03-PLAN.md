---
phase: 01-data-foundation
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - app/repositories/storage.py
  - app/ingestion/base_parser.py
  - app/ingestion/coco_parser.py
  - app/services/image_service.py
  - tests/test_images.py
  - tests/test_coco_parser.py
  - tests/fixtures/small_coco.json
  - tests/fixtures/malformed_coco.json
autonomous: true

must_haves:
  truths:
    - "StorageBackend can read files from both local filesystem and GCS paths using the same API"
    - "COCO parser streams 100K+ annotation files without loading them fully into memory"
    - "COCO parser yields batches of pandas DataFrames ready for DuckDB bulk insert"
    - "ImageService generates WebP thumbnails at 128/256/512 sizes and caches them to disk"
    - "Cached thumbnails are served on subsequent requests without regeneration"
  artifacts:
    - path: "app/repositories/storage.py"
      provides: "fsspec-based StorageBackend with read_bytes, open, exists, resolve_image_path"
      contains: "class StorageBackend"
    - path: "app/ingestion/base_parser.py"
      provides: "Abstract BaseParser interface for format-agnostic parsing"
      contains: "class BaseParser"
    - path: "app/ingestion/coco_parser.py"
      provides: "Streaming COCO parser using ijson with DataFrame batch output"
      contains: "class COCOParser"
    - path: "app/services/image_service.py"
      provides: "Thumbnail generation with disk cache, WebP output, multiple sizes"
      contains: "class ImageService"
    - path: "tests/fixtures/small_coco.json"
      provides: "10-image COCO file for fast tests"
      contains: "images"
  key_links:
    - from: "app/ingestion/coco_parser.py"
      to: "ijson"
      via: "ijson.items() with binary mode file handle and use_float=True"
      pattern: "ijson\\.items.*rb.*use_float"
    - from: "app/ingestion/coco_parser.py"
      to: "pandas"
      via: "Yields pd.DataFrame batches for DuckDB bulk insert"
      pattern: "pd\\.DataFrame"
    - from: "app/services/image_service.py"
      to: "app/repositories/storage.py"
      via: "ImageService uses StorageBackend.read_bytes to fetch original images"
      pattern: "storage\\.read_bytes"
    - from: "app/repositories/storage.py"
      to: "fsspec"
      via: "fsspec.filesystem() for local and gcs protocols"
      pattern: "fsspec\\.filesystem"
---

<objective>
Build the three core service layers: fsspec storage abstraction (local + GCS), streaming COCO parser with ijson, and image/thumbnail service with disk cache.

Purpose: These are the domain-specific engines that the ingestion pipeline (Plan 04) orchestrates. The storage backend enables transparent local/GCS access. The COCO parser handles 100K+ files without OOM. The image service generates and caches WebP thumbnails for instant browsing.
Output: Three tested, standalone services ready to be wired into the API layer.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-data-foundation/01-RESEARCH.md
@.planning/phases/01-data-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create StorageBackend (fsspec) and BaseParser interface</name>
  <files>app/repositories/storage.py, app/ingestion/base_parser.py</files>
  <action>
    **app/repositories/storage.py:** Create StorageBackend class using fsspec:

    1. `__init__(self)`: Initialize `_filesystems: dict[str, fsspec.AbstractFileSystem]` cache (lazy creation per protocol)

    2. `_get_fs(self, path: str) -> tuple[fsspec.AbstractFileSystem, str]`:
       - If path starts with "gs://": protocol = "gcs", use path as-is
       - Else: protocol = "file", resolve to absolute path via Path(path).resolve()
       - Lazily create filesystem: `fsspec.filesystem(protocol)` on first use, cache for reuse
       - Return (filesystem, normalized_path)

    3. `exists(self, path: str) -> bool`: Delegates to fs.exists()
    4. `read_bytes(self, path: str) -> bytes`: Delegates to fs.cat() -- returns full file bytes
    5. `open(self, path: str, mode: str = "rb")`: Delegates to fs.open()
    6. `list_dir(self, path: str) -> list[str]`: Delegates to fs.ls()
    7. `resolve_image_path(self, base_path: str, file_name: str) -> str`:
       - GCS: f"{base_path.rstrip('/')}/{file_name}"
       - Local: str(Path(base_path) / file_name)

    **app/ingestion/base_parser.py:** Create abstract BaseParser:

    1. `BaseParser` ABC with:
       - `format_name` abstract property (returns str like "coco", "yolo", "voc")
       - `parse_categories(self, file_path: Path) -> dict[int, str]` (abstract)
       - `build_image_batches(self, file_path: Path, dataset_id: str) -> Iterator[pd.DataFrame]` (abstract)
       - `build_annotation_batches(self, file_path: Path, dataset_id: str, categories: dict[int, str]) -> Iterator[pd.DataFrame]` (abstract)
       - `batch_size: int` constructor parameter with default 1000

    This provides the extension point for YOLO and VOC parsers in future phases.
  </action>
  <verify>
    Run `uv run python -c "
from app.repositories.storage import StorageBackend
s = StorageBackend()
# Test local file exists
import tempfile, os
with tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as f:
    f.write(b'hello')
    f.flush()
    print(f'exists: {s.exists(f.name)}')
    print(f'bytes: {s.read_bytes(f.name)}')
    os.unlink(f.name)
print('StorageBackend OK')
"` -- should print exists: True, bytes: b'hello'.
  </verify>
  <done>StorageBackend reads local files via fsspec. BaseParser defines the abstract contract for format parsers. GCS path routing is implemented (actual GCS access requires credentials).</done>
</task>

<task type="auto">
  <name>Task 2: Create streaming COCO parser with test fixtures</name>
  <files>app/ingestion/coco_parser.py, tests/test_coco_parser.py, tests/fixtures/small_coco.json, tests/fixtures/malformed_coco.json</files>
  <action>
    **tests/fixtures/small_coco.json:** Create a valid COCO format JSON file with:
    - 3 categories (e.g., "person", "car", "dog")
    - 10 images with realistic metadata (id, file_name, width, height)
    - 15-20 annotations across those images, including:
      - At least one iscrowd=1 annotation (to test RLE handling)
      - Varied bbox values
      - Multiple annotations per image

    **tests/fixtures/malformed_coco.json:** Create edge case COCO file:
    - Missing "categories" key (empty categories)
    - An image with missing "width" field
    - An annotation referencing a non-existent category_id

    **app/ingestion/coco_parser.py:** Create COCOParser(BaseParser):

    1. `format_name` property returns "coco"

    2. `parse_categories(self, file_path: Path) -> dict[int, str]`:
       - Open file in binary mode ("rb") -- CRITICAL per research (ijson needs binary)
       - Use `ijson.items(f, "categories.item")` to stream categories
       - Build {category_id: category_name} dict
       - Handle missing "categories" key gracefully (return empty dict)

    3. `parse_images_streaming(self, file_path: Path) -> Iterator[dict]`:
       - Open in "rb" mode
       - yield from `ijson.items(f, "images.item", use_float=True)`

    4. `parse_annotations_streaming(self, file_path: Path) -> Iterator[dict]`:
       - Open in "rb" mode
       - yield from `ijson.items(f, "annotations.item", use_float=True)`

    5. `build_image_batches(self, file_path: Path, dataset_id: str) -> Iterator[pd.DataFrame]`:
       - Accumulate image records into batches of self.batch_size
       - Each record: id (str(image["id"])), dataset_id, file_name, width, height, thumbnail_path=None, split=None, metadata=None
       - Handle missing fields: width/height default to 0 if missing, log warning
       - Yield pd.DataFrame(batch) when batch reaches batch_size or at end

    6. `build_annotation_batches(self, file_path: Path, dataset_id: str, categories: dict[int, str]) -> Iterator[pd.DataFrame]`:
       - Accumulate annotation records into batches
       - Each record: id (str(ann["id"])), dataset_id, sample_id (str(ann["image_id"])), category_name (lookup from categories dict, default "unknown"), bbox_x/y/w/h (from ann["bbox"], default [0,0,0,0]), area, is_crowd (bool(ann.get("iscrowd", 0))), source="ground_truth", confidence=None, metadata=None
       - Handle missing bbox gracefully (default to zeros)
       - Yield pd.DataFrame(batch)

    CRITICAL DETAILS:
    - Always open files in binary mode ("rb") -- ijson's yajl2_c backend requires bytes
    - Always use `use_float=True` -- avoids Decimal overhead for coordinates
    - DataFrame column order MUST match DuckDB table column order exactly (for `INSERT INTO table SELECT * FROM df`)

    **tests/test_coco_parser.py:** Write tests:
    1. `test_parse_categories`: Parse small_coco.json, verify 3 categories returned
    2. `test_parse_images_streaming`: Count images yielded, verify 10
    3. `test_parse_annotations_streaming`: Count annotations, verify 15-20
    4. `test_build_image_batches`: Verify DataFrames have correct columns (id, dataset_id, file_name, width, height, thumbnail_path, split, metadata)
    5. `test_build_annotation_batches`: Verify columns match schema (id, dataset_id, sample_id, category_name, bbox_x, bbox_y, bbox_w, bbox_h, area, is_crowd, source, confidence, metadata)
    6. `test_batch_size_respected`: Use batch_size=3, verify each yielded DataFrame has <= 3 rows
    7. `test_iscrowd_handled`: Verify iscrowd=1 annotation produces is_crowd=True in DataFrame
    8. `test_malformed_missing_categories`: Parse malformed_coco.json, verify empty categories dict (no crash)
    9. `test_unknown_category_id`: Verify annotation with unknown category_id gets "unknown" as category_name
  </action>
  <verify>
    Run `uv run pytest tests/test_coco_parser.py -v` -- all tests pass.
    Run `uv run python -c "
from pathlib import Path
from app.ingestion.coco_parser import COCOParser
parser = COCOParser(batch_size=5)
cats = parser.parse_categories(Path('tests/fixtures/small_coco.json'))
print(f'Categories: {cats}')
batches = list(parser.build_image_batches(Path('tests/fixtures/small_coco.json'), 'test-dataset'))
print(f'Image batches: {len(batches)}, total rows: {sum(len(b) for b in batches)}')
"` -- should show categories and 10 total image rows.
  </verify>
  <done>COCOParser streams COCO JSON via ijson in binary mode with use_float=True. Yields pandas DataFrames in configurable batches. Handles iscrowd, missing categories, unknown category IDs. All 9+ tests pass.</done>
</task>

<task type="auto">
  <name>Task 3: Create ImageService with thumbnail generation and disk cache</name>
  <files>app/services/image_service.py, tests/test_images.py, tests/fixtures/sample_images/test_image_01.jpg</files>
  <action>
    **tests/fixtures/sample_images/test_image_01.jpg:** Create a small test JPEG image programmatically in the test setup (or generate via Pillow in conftest). The test fixture should be a real 100x100 JPEG file.

    **app/services/image_service.py:** Create ImageService class:

    1. Constants at module level:
       - THUMBNAIL_SIZES = {"small": 128, "medium": 256, "large": 512}
       - DEFAULT_THUMBNAIL_SIZE = "medium"
       - WEBP_QUALITY = 80

    2. `__init__(self, cache_dir: Path, storage: StorageBackend)`:
       - Store cache_dir and storage
       - Create cache_dir if it doesn't exist: cache_dir.mkdir(parents=True, exist_ok=True)

    3. `get_cache_path(self, sample_id: str, size: str) -> Path`:
       - width = THUMBNAIL_SIZES.get(size, THUMBNAIL_SIZES[DEFAULT_THUMBNAIL_SIZE])
       - Return cache_dir / f"{sample_id}_{width}.webp"

    4. `get_or_generate_thumbnail(self, sample_id: str, image_path: str, size: str = "medium") -> Path`:
       - Check cache_path -- if exists, return immediately (cache hit)
       - Read original image bytes via self.storage.read_bytes(image_path)
       - Open with PIL.Image.open(BytesIO(image_bytes))
       - Get target width from THUMBNAIL_SIZES
       - Call img.thumbnail((width, width), Image.Resampling.LANCZOS) -- preserves aspect ratio
       - Convert to RGB if mode is RGBA or P (WebP doesn't support all modes)
       - Save as WebP: img.save(cache_path, format="WEBP", quality=WEBP_QUALITY, method=4)
       - Return cache_path

    5. `generate_thumbnails_batch(self, samples: list[dict], size: str = "medium") -> tuple[int, int]`:
       - Iterate samples, each with keys "id" and "image_path"
       - For each, call get_or_generate_thumbnail inside try/except
       - Count successes and failures
       - Return (generated_count, error_count) -- never fail on individual thumbnail errors

    IMPORTANT: Method 4 for WebP save is the best speed/quality tradeoff (method 6 is highest quality but slowest). Use Image.Resampling.LANCZOS (NOT the deprecated Image.ANTIALIAS).

    **tests/test_images.py:** Write tests:
    1. `test_thumbnail_generation`: Create a test image (via Pillow in test), generate thumbnail, verify .webp file exists at cache_path
    2. `test_thumbnail_cache_hit`: Generate thumbnail, call again, verify second call returns same path without regeneration (check file mtime doesn't change)
    3. `test_thumbnail_sizes`: Generate small, medium, large thumbnails, verify each file has different pixel dimensions
    4. `test_thumbnail_rgb_conversion`: Create RGBA PNG test image, generate thumbnail, verify output is valid WebP
    5. `test_batch_generation`: Generate thumbnails for 3 test images, verify (3, 0) returned
    6. `test_batch_error_isolation`: Include a non-existent image path in batch, verify other thumbnails still generate (count, errors) returned correctly
    7. `test_cache_path_deterministic`: Same sample_id + size always produces same cache path

    Use tmp_path fixture for cache_dir. Create test images programmatically with Pillow (don't rely on external files):
    ```python
    from PIL import Image
    img = Image.new("RGB", (800, 600), color="red")
    img.save(path, "JPEG")
    ```
  </action>
  <verify>
    Run `uv run pytest tests/test_images.py -v` -- all tests pass.
    Run `uv run python -c "
from pathlib import Path
from PIL import Image
from app.repositories.storage import StorageBackend
from app.services.image_service import ImageService
# Create test image
img = Image.new('RGB', (800, 600), 'blue')
img.save('/tmp/test_img.jpg', 'JPEG')
# Generate thumbnail
svc = ImageService(Path('/tmp/thumb_cache'), StorageBackend())
thumb = svc.get_or_generate_thumbnail('sample-1', '/tmp/test_img.jpg', 'medium')
print(f'Thumbnail: {thumb}, exists: {thumb.exists()}')
t = Image.open(thumb)
print(f'Size: {t.size}, format: {t.format}')
"` -- should show a 256xN WebP thumbnail.
  </verify>
  <done>ImageService generates WebP thumbnails with LANCZOS resampling at 128/256/512 sizes. Disk cache prevents regeneration. Batch generation is error-isolated. All 7+ tests pass.</done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_coco_parser.py tests/test_images.py -v` -- all tests pass
2. COCOParser reads small_coco.json and yields correct number of images and annotations in DataFrames
3. DataFrame column order matches DuckDB schema exactly (verified by column name comparison in tests)
4. StorageBackend reads local files correctly
5. ImageService generates thumbnails from test images, caches to disk, serves cached on second call
6. All edge cases handled: iscrowd, missing categories, RGBA conversion, batch errors
</verification>

<success_criteria>
- StorageBackend provides unified local/GCS file access via fsspec
- COCOParser streams COCO JSON with ijson (binary mode, use_float=True) and yields DataFrame batches
- ImageService generates WebP thumbnails at 3 sizes with disk cache
- All services are tested with edge cases covered
- DataFrame columns match DuckDB table schemas from Plan 01
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-foundation/01-03-SUMMARY.md`
</output>
