---
phase: 01-data-foundation
plan: 04
type: execute
wave: 3
depends_on: ["01-01", "01-02", "01-03"]
files_modified:
  - app/services/ingestion.py
  - app/routers/datasets.py
  - app/routers/samples.py
  - app/routers/images.py
  - app/main.py
  - app/dependencies.py
  - tests/test_ingestion.py
  - tests/test_samples_api.py
autonomous: true

must_haves:
  truths:
    - "User can POST /datasets/ingest with a COCO JSON path and image directory, and receive SSE progress events"
    - "After ingestion, GET /datasets returns the ingested dataset with correct image_count and annotation_count"
    - "GET /samples?dataset_id=X returns paginated samples with correct metadata"
    - "GET /images/{dataset_id}/{sample_id}?size=medium returns a cached WebP thumbnail"
    - "Plugin hooks fire during ingestion (on_ingest_start, on_sample_ingested, on_ingest_complete)"
    - "Thumbnails are generated for the first batch of images during ingestion"
  artifacts:
    - path: "app/services/ingestion.py"
      provides: "IngestionService orchestrating parser, DB inserts, thumbnails, and plugin hooks"
      contains: "class IngestionService"
    - path: "app/routers/datasets.py"
      provides: "POST /datasets/ingest (SSE), GET /datasets, GET /datasets/{id}"
      contains: "router"
    - path: "app/routers/samples.py"
      provides: "GET /samples with pagination and filtering"
      contains: "router"
    - path: "app/routers/images.py"
      provides: "GET /images/{dataset_id}/{sample_id} with thumbnail serving"
      contains: "router"
    - path: "tests/test_ingestion.py"
      provides: "End-to-end ingestion test with progress tracking"
      contains: "test_ingest"
  key_links:
    - from: "app/services/ingestion.py"
      to: "app/ingestion/coco_parser.py"
      via: "IngestionService creates COCOParser and iterates build_image_batches/build_annotation_batches"
      pattern: "COCOParser.*build_image_batches"
    - from: "app/services/ingestion.py"
      to: "app/repositories/duckdb_repo.py"
      via: "cursor.execute INSERT INTO samples/annotations SELECT * FROM df"
      pattern: "INSERT INTO.*SELECT \\* FROM"
    - from: "app/services/ingestion.py"
      to: "app/plugins/registry.py"
      via: "trigger_hook calls for ingestion lifecycle events"
      pattern: "trigger_hook.*on_ingest"
    - from: "app/services/ingestion.py"
      to: "app/services/image_service.py"
      via: "generate_thumbnails_batch for first N images post-insert"
      pattern: "generate_thumbnails_batch"
    - from: "app/routers/datasets.py"
      to: "app/services/ingestion.py"
      via: "POST /datasets/ingest calls ingest_with_progress as SSE stream"
      pattern: "StreamingResponse.*text/event-stream"
    - from: "app/routers/images.py"
      to: "app/services/image_service.py"
      via: "GET /images/ delegates to get_or_generate_thumbnail"
      pattern: "get_or_generate_thumbnail"
    - from: "app/main.py"
      to: "app/routers/datasets.py"
      via: "app.include_router(datasets.router)"
      pattern: "include_router"
---

<objective>
Wire all services into the API layer: IngestionService orchestrates parsing + DB inserts + thumbnails + plugin hooks, exposed via SSE endpoint. Add datasets, samples, and images routers. Complete the FastAPI app.

Purpose: This is the integration plan that makes Phase 1 functional end-to-end. After this plan, a user can POST a COCO dataset, watch progress stream in, and query the results via API.
Output: A fully functional ingestion pipeline with 3 API routers and end-to-end tests.
</objective>

<execution_context>
@/Users/ortizeg/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ortizeg/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-data-foundation/01-RESEARCH.md
@.planning/phases/01-data-foundation/01-01-SUMMARY.md
@.planning/phases/01-data-foundation/01-02-SUMMARY.md
@.planning/phases/01-data-foundation/01-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create IngestionService with progress streaming and wire dependencies</name>
  <files>app/services/ingestion.py, app/dependencies.py, app/main.py</files>
  <action>
    **app/services/ingestion.py:** Create IngestionService class:

    1. `IngestionProgress` dataclass:
       - stage: str (one of: "categories", "parsing_images", "parsing_annotations", "thumbnails", "complete")
       - current: int
       - total: int | None (None when streaming and total is unknown)
       - message: str

    2. `__init__(self, db: DuckDBRepo, storage: StorageBackend, image_service: ImageService, plugin_registry: PluginRegistry)`:
       - Store all dependencies

    3. `ingest_with_progress(self, annotation_path: str, image_dir: str, dataset_name: str | None = None, format: str = "coco") -> Iterator[IngestionProgress]`:
       - This is a synchronous generator (not async) -- FastAPI will wrap it in StreamingResponse
       - Generate dataset_id = str(uuid.uuid4())
       - Derive name from dataset_name or Path(annotation_path).stem

       Step 1 - Parse categories:
       - parser = COCOParser(batch_size=1000)
       - categories = parser.parse_categories(Path(annotation_path))
       - Insert categories into DuckDB: build DataFrame from categories dict, execute INSERT INTO categories SELECT * FROM cat_df
       - yield progress(stage="categories", current=len(categories), total=len(categories))

       Step 2 - Stream and insert images:
       - Create cursor from self.db.connection.cursor()
       - For each batch_df from parser.build_image_batches():
         - cursor.execute("INSERT INTO samples SELECT * FROM batch_df")
         - Trigger plugin hook: on_sample_ingested for each row (optional -- only if plugins registered, iterate rows in batch)
         - image_count += len(batch_df)
         - yield progress(stage="parsing_images", current=image_count, total=None)

       Step 3 - Stream and insert annotations:
       - For each batch_df from parser.build_annotation_batches():
         - cursor.execute("INSERT INTO annotations SELECT * FROM batch_df")
         - ann_count += len(batch_df)
         - yield progress(stage="parsing_annotations", current=ann_count, total=None)

       Step 4 - Insert dataset record:
       - cursor.execute("INSERT INTO datasets VALUES (?, ?, ?, ?, ?, ?, ?, ?, current_timestamp, NULL)", [dataset_id, name, format, annotation_path, image_dir, image_count, ann_count, len(categories)])

       Step 5 - Generate thumbnails (hybrid approach from research):
       - Query first 500 samples: SELECT id, file_name FROM samples WHERE dataset_id = ? LIMIT 500
       - For each sample, resolve full image path via storage.resolve_image_path(image_dir, file_name)
       - Call image_service.generate_thumbnails_batch(samples_with_paths, "medium")
       - yield progress(stage="thumbnails", current=generated, total=min(500, image_count))
       - Note: remaining thumbnails will be generated on-demand when accessed via the images endpoint

       Step 6 - Trigger plugin hooks:
       - self.plugins.trigger_hook("on_ingest_start", context=PluginContext(dataset_id=dataset_id)) at the beginning
       - self.plugins.trigger_hook("on_ingest_complete", context=PluginContext(dataset_id=dataset_id), stats={...}) at the end

       Step 7 - Final progress:
       - yield progress(stage="complete", current=image_count, total=image_count, message=f"Ingestion complete: {image_count} images, {ann_count} annotations")
       - Close cursor in finally block

       IMPORTANT: The INSERT INTO table SELECT * FROM df pattern requires the DataFrame variable name to be visible in the cursor's scope. DuckDB resolves Python variable names in the SQL. Make sure the variable is named `batch_df` (or whatever name appears in the SQL string).

    **app/dependencies.py:** Update the placeholder stubs from Plan 01:
    - `get_storage(request: Request) -> StorageBackend`: Return request.app.state.storage
    - `get_image_service(request: Request) -> ImageService`: Return request.app.state.image_service
    - `get_plugin_registry(request: Request) -> PluginRegistry`: Return request.app.state.plugin_registry
    - `get_ingestion_service(db, storage, image_service, plugin_registry) -> IngestionService`: Compose from other deps

    **app/main.py:** Update lifespan to initialize ALL services:
    - Create StorageBackend(), store on app.state.storage
    - Create ImageService(settings.thumbnail_cache_dir, storage), store on app.state.image_service
    - Create PluginRegistry(), discover_plugins(settings.plugin_dir), store on app.state.plugin_registry
    - Uncomment/add router includes for datasets, samples, images
    - On shutdown: call plugin_registry.shutdown(), then db.close()
  </action>
  <verify>
    Run `uv run python -c "
from app.services.ingestion import IngestionService, IngestionProgress
print('IngestionService importable')
from app.dependencies import get_storage, get_image_service, get_plugin_registry
print('Dependencies importable')
"` -- should print both messages.
    Run `uv run uvicorn app.main:app --port 8000` -- app starts with all services initialized, logs plugin discovery.
  </verify>
  <done>IngestionService orchestrates COCO parsing, DuckDB bulk inserts, thumbnail generation, and plugin hooks. All dependencies wired in lifespan and available via FastAPI DI. App starts with all services.</done>
</task>

<task type="auto">
  <name>Task 2: Create API routers (datasets, samples, images) and end-to-end tests</name>
  <files>app/routers/datasets.py, app/routers/samples.py, app/routers/images.py, tests/test_ingestion.py, tests/test_samples_api.py</files>
  <action>
    **app/routers/datasets.py:** Create datasets router:

    1. `POST /datasets/ingest`:
       - Accept IngestRequest body
       - Create IngestionService from dependencies
       - Return StreamingResponse with media_type="text/event-stream"
       - Stream function iterates ingest_with_progress(), yields `data: {json}\n\n` for each IngestionProgress
       - Final event: `data: {"stage": "complete", ...}\n\n`
       - Headers: Cache-Control: no-cache, Connection: keep-alive, X-Accel-Buffering: no

    2. `GET /datasets`:
       - Query all datasets from DuckDB: SELECT * FROM datasets ORDER BY created_at DESC
       - Return DatasetListResponse

    3. `GET /datasets/{dataset_id}`:
       - Query single dataset: SELECT * FROM datasets WHERE id = ?
       - Return DatasetResponse or 404

    4. `DELETE /datasets/{dataset_id}`:
       - Delete dataset and all related samples, annotations, categories
       - DELETE FROM annotations WHERE dataset_id = ?
       - DELETE FROM samples WHERE dataset_id = ?
       - DELETE FROM categories WHERE dataset_id = ?
       - DELETE FROM datasets WHERE id = ?
       - Return 204 No Content

    **app/routers/samples.py:** Create samples router:

    1. `GET /samples`:
       - Query params: dataset_id (required), category (optional), split (optional), offset (int = 0), limit (int = 50, max 200)
       - Build dynamic WHERE clause from provided filters
       - For category filter: JOIN annotations ON samples.id = annotations.sample_id WHERE annotations.category_name = ?
       - Execute count query and data query
       - Return PaginatedSamples with items, total, offset, limit

    2. `GET /samples/{sample_id}/annotations`:
       - Query params: dataset_id (required)
       - Return list of AnnotationResponse for the sample

    **app/routers/images.py:** Create images router:

    1. `GET /images/{dataset_id}/{sample_id}`:
       - Query param: size (str = "medium", one of "small", "medium", "large", "original")
       - Look up sample in DuckDB: SELECT file_name FROM samples WHERE id = ? AND dataset_id = ?
       - Look up dataset for image_dir: SELECT image_dir FROM datasets WHERE id = ?
       - 404 if not found
       - If size == "original":
         - Resolve full path via storage.resolve_image_path(image_dir, file_name)
         - For local files: return FileResponse
         - For GCS (gs://): read bytes via storage.read_bytes(), return Response(content=bytes, media_type="image/jpeg")
       - For thumbnails:
         - Resolve image_path, call image_service.get_or_generate_thumbnail(sample_id, image_path, size)
         - Return FileResponse(str(thumbnail_path), media_type="image/webp")

    **tests/test_ingestion.py:** End-to-end ingestion tests:
    1. `test_ingest_coco_dataset`: Use test fixtures (small_coco.json + sample images).
       - Create a temporary directory with small_coco.json and 10 test images (generate via Pillow)
       - POST /datasets/ingest with annotation_path and image_dir
       - Read SSE events, verify stages: categories -> parsing_images -> parsing_annotations -> thumbnails -> complete
       - Verify final event has correct image_count and annotation_count
    2. `test_get_datasets_after_ingest`: After ingestion, GET /datasets returns 1 dataset with correct counts
    3. `test_get_dataset_by_id`: GET /datasets/{id} returns the specific dataset
    4. `test_delete_dataset`: DELETE /datasets/{id} returns 204, GET /datasets returns empty

    **tests/test_samples_api.py:** Sample and image API tests:
    1. `test_get_samples_paginated`: After ingestion, GET /samples?dataset_id=X returns paginated results
    2. `test_get_samples_with_limit`: Verify limit parameter works (request limit=3, get 3 items)
    3. `test_get_sample_annotations`: GET /samples/{id}/annotations returns annotation list
    4. `test_get_image_thumbnail`: GET /images/{dataset_id}/{sample_id}?size=medium returns WebP image
    5. `test_get_image_not_found`: GET /images/{dataset_id}/nonexistent returns 404

    For tests that need ingestion, create a pytest fixture that runs the ingestion service directly (not through API) to avoid SSE parsing complexity in tests. Use the IngestionService.ingest_with_progress() generator and consume it fully.

    IMPORTANT for SSE test: httpx does not natively handle SSE. For the POST /datasets/ingest test, use `httpx.AsyncClient.stream("POST", ...)` and iterate response.aiter_lines() to read SSE events. Alternatively, call the ingestion service directly and test the API endpoint separately with a mocked/pre-ingested dataset.
  </action>
  <verify>
    Run `uv run pytest tests/test_ingestion.py tests/test_samples_api.py -v` -- all tests pass.
    Manual integration test:
    1. Start server: `uv run uvicorn app.main:app --port 8000`
    2. Run: `curl -X POST http://localhost:8000/datasets/ingest -H "Content-Type: application/json" -d '{"annotation_path": "tests/fixtures/small_coco.json", "image_dir": "tests/fixtures/sample_images"}' -N` -- should stream SSE events
    3. Run: `curl http://localhost:8000/datasets` -- should return dataset list
    4. Run: `curl "http://localhost:8000/samples?dataset_id=<ID>&limit=5"` -- should return paginated samples
  </verify>
  <done>
    POST /datasets/ingest streams SSE progress and completes ingestion. GET /datasets, /samples, /images endpoints work. All API tests pass. End-to-end flow verified: ingest -> query datasets -> query samples -> serve thumbnails.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/ -v` -- ALL tests across all plans pass (test_plugins, test_coco_parser, test_images, test_ingestion, test_samples_api)
2. Full manual integration test:
   - Start server with `uv run uvicorn app.main:app --port 8000 --workers 1`
   - POST /datasets/ingest with small_coco.json fixture -- SSE events stream correctly
   - GET /datasets returns the ingested dataset
   - GET /samples?dataset_id=X returns paginated samples
   - GET /images/{dataset_id}/{sample_id}?size=medium returns a WebP thumbnail
3. Plugin hooks fire during ingestion (example plugin logs visible in server output)
4. DuckDB contains correct data: datasets (1 row), samples (10 rows), annotations (15-20 rows), categories (3 rows)
</verification>

<success_criteria>
- IngestionService orchestrates streaming parse -> bulk insert -> thumbnail generation -> plugin hooks
- SSE progress endpoint streams real-time updates during ingestion
- Datasets, samples, images API routers serve correct data
- Thumbnail serving works with on-demand generation fallback
- Plugin hooks integrated into ingestion lifecycle
- All tests pass including end-to-end ingestion test
- Phase 1 success criteria met:
  1. COCO ingestion works with streaming parser (no OOM)
  2. Local filesystem images served via same API (GCS via same storage abstraction)
  3. Thumbnails cached to disk during ingestion
  4. All metadata queryable in DuckDB via API
  5. BasePlugin exists with extension points
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-foundation/01-04-SUMMARY.md`
</output>
